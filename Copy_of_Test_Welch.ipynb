{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Test Welch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "gb3rAg49Reog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kkls5uowRir0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ada-A9jASVlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AZj1yWWSSXmX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#2. Get the files\n",
        "feature_downloaded = drive.CreateFile({'id':'1X8p6OHC-3AHxmlGuw9bGFSqgGP2au1CK'})\n",
        "feature_downloaded.GetContentFile('emotiv_7sub_5class.mat')\n",
        "\n",
        "all_downloaded = drive.CreateFile({'id':'10p_NuiBV2Or2sk6cm0yPLfu9tJ2lXEKg'})\n",
        "all_downloaded.GetContentFile('S1_nolabel6.mat')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZWCCTrlVSZhJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import scipy.io as sc\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# this function is used to transfer one column label to one hot label\n",
        "def one_hot(y_):\n",
        "    # Function to encode output labels from number indexes\n",
        "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
        "    y_ = y_.reshape(len(y_))\n",
        "    n_values = np.max(y_) + 1\n",
        "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  Data loading\n",
        "feature = sc.loadmat(\"S1_nolabel6.mat\")\n",
        "all = feature['S1_nolabel6']\n",
        "print('Feature')\n",
        "print(feature)\n",
        "\n",
        "print('shape')\n",
        "\n",
        "print (all.shape)\n",
        "\n",
        "np.random.shuffle(all)   # mix eeg_all\n",
        "# Get the 28000 samples of that subject\n",
        "final=2800*10\n",
        "all=all[0:final]\n",
        "\n",
        "# Get the features\n",
        "feature_all =all[:,0:64]\n",
        "# Get the label\n",
        "label=all[:,64:65]\n",
        "\n",
        "# z-score\n",
        "\n",
        "print(\"Feature All\")\n",
        "print(feature_all)\n",
        "print(feature_all.shape)\n",
        "no_fea=feature_all.shape[-1]\n",
        "label_all=one_hot(label)\n",
        "print(\"\")\n",
        "print (label_all.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6GcCHoLS-T5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import signal\n",
        "sns.set(font_scale=1.2)\n",
        "print(\"before\")\n",
        "print(feature_all)\n",
        "feature_all=preprocessing.scale(feature_all)\n",
        "print(\"After\")\n",
        "print(feature_all)\n",
        "\n",
        "data = feature_all\n",
        "\n",
        "# Define sampling frequency and time vector\n",
        "sf = 160\n",
        "time = np.arange(data.shape[0]) / sf\n",
        "print('data')\n",
        "print(data.shape)\n",
        "print('time')\n",
        "print(time.shape)\n",
        "# Plot the signal\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
        "plt.plot(time, data, lw=1.5, color='k')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Voltage')\n",
        "plt.xlim([time.min(), time.max()])\n",
        "plt.title('EEG Data')\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcuCvBfagbUX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # Define window length (4 seconds)\n",
        "win = 0.5 * sf\n",
        "freqs, psd = signal.welch(data, sf, nperseg=win)\n",
        "print('freqs')\n",
        "print(freqs)\n",
        "print('psd')\n",
        "print(psd.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jcXwAnEYrhMn",
        "colab_type": "code",
        "outputId": "4b6b0d7b-8e04-4e22-c0fb-765448d6a42d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5732
        }
      },
      "cell_type": "code",
      "source": [
        "n_classes=6\n",
        "###CNN code,\n",
        "feature_all=feature_all# the input data of CNN\n",
        "print (\"cnn input feature shape\", feature_all.shape)\n",
        "n_fea=feature_all.shape[-1]\n",
        "print(n_fea)\n",
        "# label_all=one_hot(label_all)\n",
        "\n",
        "final=all.shape[0]\n",
        "middle_number=final*3/4\n",
        "print(\"-----\",middle_number)\n",
        "feature_training =feature_all[0:middle_number]\n",
        "feature_testing =feature_all[middle_number:final]\n",
        "label_training =label_all[0:middle_number]\n",
        "label_testing =label_all[middle_number:final]\n",
        "label_ww=label_all[middle_number:final]  # for the confusion matrix\n",
        "print (\"label_testing\",label_testing.shape)\n",
        "a=feature_training\n",
        "b=feature_testing\n",
        "print(feature_training.shape)\n",
        "print(feature_testing.shape)\n",
        "\n",
        "keep=1\n",
        "batch_size=final-middle_number\n",
        "n_group=3\n",
        "train_fea=[]\n",
        "for i in range(n_group):\n",
        "    f =a[(0+batch_size*i):(batch_size+batch_size*i)]\n",
        "    train_fea.append(f)\n",
        "print(\"Here\")\n",
        "print (train_fea[0].shape)\n",
        "\n",
        "train_label=[]\n",
        "for i in range(n_group):\n",
        "    f =label_training[(0+batch_size*i):(batch_size+batch_size*i), :]\n",
        "    train_label.append(f)\n",
        "print (train_label[0].shape)\n",
        "\n",
        "# the CNN code\n",
        "def compute_accuracy(v_xs, v_ys):\n",
        "    global prediction\n",
        "    y_pre = sess3.run(prediction, feed_dict={xs: v_xs, keep_prob: keep})\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    result = sess3.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: keep})\n",
        "    return result\n",
        "\n",
        "#to creat a random weights \n",
        "def weight_variable(shape):\n",
        "    # Outputs random values from a truncated normal distribution\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    # A variable maintains state in the graph across calls to run(). \n",
        "    # You add a variable to the graph by constructing an instance of the class Variable.\n",
        "    print('shape')\n",
        "    print(shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "#random bias values\n",
        "def bias_variable(shape):\n",
        "    # Creates a constant tensor\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    # stride [1, x_movement, y_movement, 1]\n",
        "    # Must have strides[0] = strides[3] = 1\n",
        "    # the concolution layer x is the input\n",
        "    # w is the weight and the stride is how many moves it makes in each dimention ie how many pixels\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "# def max_pool_2x2(x):\n",
        "#     # stride [1, x_movement, y_movement, 1]\n",
        "#     return tf.nn.max_pool(x, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "#max pooling to reduce dimentionality .. here consider every 1*2 window\n",
        "def max_pool_1x2(x):\n",
        "    # stride [1, x_movement, y_movement, 1]\n",
        "    return tf.nn.max_pool(x, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "\n",
        "# define placeholder for inputs to network\n",
        "xs = tf.placeholder(tf.float32, [None, n_fea]) # 1*64\n",
        "ys = tf.placeholder(tf.float32, [None, n_classes])  # 2 is the classes of the data\n",
        "# Lookup what is keep_prob\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "x_image = tf.reshape(xs, [-1, 1, n_fea, 1])\n",
        "print('xs')\n",
        "print(xs)\n",
        "print(xs.shape)\n",
        "print('x_image')\n",
        "print(x_image)\n",
        "print(x_image.shape)  \n",
        "\n",
        "## conv1 layer ##\n",
        "W_conv1 = weight_variable([1,1, 1,20]) # patch 1*1, in size is 1, out size is 2\n",
        "b_conv1 = bias_variable([20])\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 1*64*2\n",
        "h_pool1 = max_pool_1x2(h_conv1)                          # output size 1*32x2\n",
        "\n",
        "## conv2 layer ##\n",
        "# W_conv2 = weight_variable([1,1, 2, 4]) # patch 1*1, in size 2, out size 4\n",
        "# b_conv2 = bias_variable([4])\n",
        "# h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 1*32*4\n",
        "# h_pool2 = max_pool_1x2(h_conv2)                          # output size 1*16*4\n",
        "\n",
        "## fc1 layer ## fc fully connected layer\n",
        "W_fc1 = weight_variable([1*(n_fea/2)*20, 120])\n",
        "b_fc1 = bias_variable([120])\n",
        "# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]\n",
        "h_pool2_flat = tf.reshape(h_pool1, [-1, 1*(n_fea/2)*20])\n",
        "h_fc1 = tf.nn.sigmoid(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "## fc2 layer ##\n",
        "W_fc2 = weight_variable([120, n_classes])\n",
        "b_fc2 = bias_variable([n_classes])\n",
        "# Multiplies matrix a by matrix b, producing a * b\n",
        "prediction = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
        "\n",
        "# Weight regulrization\n",
        "l2 = 0.001 * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "# Getting the mean of the errors between the predication results and the class labels in the trainning data\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=ys))+l2   # Softmax loss\n",
        "# Using optimizer\n",
        "train_step = tf.train.AdamOptimizer(0.04).minimize(cross_entropy)\n",
        "# Begin session to visit the nodes (tensors) of the graph\n",
        "sess3 = tf.Session()\n",
        "# Initializae all the defined variables\n",
        "init = tf.global_variables_initializer()\n",
        "# Visit the nodes of those variables\n",
        "sess3.run(init)\n",
        "# Total number of array elements which trigger summarization rather than full array\n",
        "#np.set_printoptions(threshold=np.nan)\n",
        "step = 1\n",
        "while step < 1500:\n",
        "    # Train the model\n",
        "    for i in range(n_group):\n",
        "        sess3.run(train_step, feed_dict={xs: train_fea[i], ys: train_label[i], keep_prob:keep})\n",
        "    # After 5 steps, use the model on the test data\n",
        "    if step % 5 == 0:\n",
        "        # Compute the cost using the cross entropy\n",
        "        cost=sess3.run(cross_entropy, feed_dict={xs: b, ys: label_testing, keep_prob: keep})\n",
        "        # Compute the accuracy\n",
        "        acc_cnn_t=compute_accuracy(b, label_testing)\n",
        "        print('the step is:',step,',the acc is',acc_cnn_t,', the cost is', cost)\n",
        "    step+=1\n",
        "acc_cnn=compute_accuracy(b, label_testing)\n",
        "feature_all_cnn=sess3.run(h_fc1_drop, feed_dict={xs: feature_all, keep_prob: keep})\n",
        "print (\"the shape of cnn output features\",feature_all.shape,label_all.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('cnn input feature shape', (28000, 64))\n",
            "64\n",
            "('-----', 21000)\n",
            "('label_testing', (7000, 6))\n",
            "(21000, 64)\n",
            "(7000, 64)\n",
            "Here\n",
            "(7000, 64)\n",
            "(7000, 6)\n",
            "xs\n",
            "Tensor(\"Placeholder:0\", shape=(?, 64), dtype=float32)\n",
            "(?, 64)\n",
            "x_image\n",
            "Tensor(\"Reshape:0\", shape=(?, 1, 64, 1), dtype=float32)\n",
            "(?, 1, 64, 1)\n",
            "shape\n",
            "[1, 1, 1, 20]\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "shape\n",
            "[640, 120]\n",
            "WARNING:tensorflow:From <ipython-input-8-ce26d3f7470a>:110: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "shape\n",
            "[120, 6]\n",
            "WARNING:tensorflow:From <ipython-input-8-ce26d3f7470a>:121: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "('the step is:', 5, ',the acc is', 0.34857142, ', the cost is', 1.9032505)\n",
            "('the step is:', 10, ',the acc is', 0.35857144, ', the cost is', 1.5726992)\n",
            "('the step is:', 15, ',the acc is', 0.381, ', the cost is', 1.510652)\n",
            "('the step is:', 20, ',the acc is', 0.45342857, ', the cost is', 1.4276532)\n",
            "('the step is:', 25, ',the acc is', 0.456, ', the cost is', 1.4412621)\n",
            "('the step is:', 30, ',the acc is', 0.46228573, ', the cost is', 1.3757945)\n",
            "('the step is:', 35, ',the acc is', 0.4732857, ', the cost is', 1.3885742)\n",
            "('the step is:', 40, ',the acc is', 0.4787143, ', the cost is', 1.341586)\n",
            "('the step is:', 45, ',the acc is', 0.5072857, ', the cost is', 1.3467996)\n",
            "('the step is:', 50, ',the acc is', 0.4452857, ', the cost is', 1.5164922)\n",
            "('the step is:', 55, ',the acc is', 0.533, ', the cost is', 1.3179405)\n",
            "('the step is:', 60, ',the acc is', 0.515, ', the cost is', 1.2984079)\n",
            "('the step is:', 65, ',the acc is', 0.5221428, ', the cost is', 1.3014219)\n",
            "('the step is:', 70, ',the acc is', 0.56871426, ', the cost is', 1.2356957)\n",
            "('the step is:', 75, ',the acc is', 0.48414287, ', the cost is', 1.4524858)\n",
            "('the step is:', 80, ',the acc is', 0.5368571, ', the cost is', 1.3509201)\n",
            "('the step is:', 85, ',the acc is', 0.549, ', the cost is', 1.2765998)\n",
            "('the step is:', 90, ',the acc is', 0.5337143, ', the cost is', 1.3106081)\n",
            "('the step is:', 95, ',the acc is', 0.56871426, ', the cost is', 1.2378813)\n",
            "('the step is:', 100, ',the acc is', 0.53257143, ', the cost is', 1.3051666)\n",
            "('the step is:', 105, ',the acc is', 0.55414283, ', the cost is', 1.2733177)\n",
            "('the step is:', 110, ',the acc is', 0.57914287, ', the cost is', 1.201241)\n",
            "('the step is:', 115, ',the acc is', 0.5608571, ', the cost is', 1.2504706)\n",
            "('the step is:', 120, ',the acc is', 0.548, ', the cost is', 1.2647729)\n",
            "('the step is:', 125, ',the acc is', 0.5942857, ', the cost is', 1.2066796)\n",
            "('the step is:', 130, ',the acc is', 0.5575714, ', the cost is', 1.2959527)\n",
            "('the step is:', 135, ',the acc is', 0.59085715, ', the cost is', 1.2167078)\n",
            "('the step is:', 140, ',the acc is', 0.60057145, ', the cost is', 1.1818504)\n",
            "('the step is:', 145, ',the acc is', 0.602, ', the cost is', 1.2013719)\n",
            "('the step is:', 150, ',the acc is', 0.58585715, ', the cost is', 1.2324862)\n",
            "('the step is:', 155, ',the acc is', 0.61542857, ', the cost is', 1.1705283)\n",
            "('the step is:', 160, ',the acc is', 0.5831429, ', the cost is', 1.2209697)\n",
            "('the step is:', 165, ',the acc is', 0.58671427, ', the cost is', 1.2439425)\n",
            "('the step is:', 170, ',the acc is', 0.60657144, ', the cost is', 1.1821641)\n",
            "('the step is:', 175, ',the acc is', 0.61457145, ', the cost is', 1.1845626)\n",
            "('the step is:', 180, ',the acc is', 0.60814285, ', the cost is', 1.1864532)\n",
            "('the step is:', 185, ',the acc is', 0.60342854, ', the cost is', 1.2100596)\n",
            "('the step is:', 190, ',the acc is', 0.59557146, ', the cost is', 1.2281755)\n",
            "('the step is:', 195, ',the acc is', 0.6315714, ', the cost is', 1.1509514)\n",
            "('the step is:', 200, ',the acc is', 0.6247143, ', the cost is', 1.1701411)\n",
            "('the step is:', 205, ',the acc is', 0.6464286, ', the cost is', 1.1188887)\n",
            "('the step is:', 210, ',the acc is', 0.55614287, ', the cost is', 1.3200219)\n",
            "('the step is:', 215, ',the acc is', 0.63285714, ', the cost is', 1.173482)\n",
            "('the step is:', 220, ',the acc is', 0.6522857, ', the cost is', 1.1126095)\n",
            "('the step is:', 225, ',the acc is', 0.55614287, ', the cost is', 1.3021512)\n",
            "('the step is:', 230, ',the acc is', 0.6035714, ', the cost is', 1.2285929)\n",
            "('the step is:', 235, ',the acc is', 0.6357143, ', the cost is', 1.1541748)\n",
            "('the step is:', 240, ',the acc is', 0.6572857, ', the cost is', 1.1047943)\n",
            "('the step is:', 245, ',the acc is', 0.54085714, ', the cost is', 1.3936261)\n",
            "('the step is:', 250, ',the acc is', 0.6407143, ', the cost is', 1.1871896)\n",
            "('the step is:', 255, ',the acc is', 0.62385714, ', the cost is', 1.1771597)\n",
            "('the step is:', 260, ',the acc is', 0.6297143, ', the cost is', 1.1748186)\n",
            "('the step is:', 265, ',the acc is', 0.6424286, ', the cost is', 1.1265194)\n",
            "('the step is:', 270, ',the acc is', 0.65485716, ', the cost is', 1.1076905)\n",
            "('the step is:', 275, ',the acc is', 0.6092857, ', the cost is', 1.2406542)\n",
            "('the step is:', 280, ',the acc is', 0.64257145, ', the cost is', 1.1491575)\n",
            "('the step is:', 285, ',the acc is', 0.66985714, ', the cost is', 1.0957325)\n",
            "('the step is:', 290, ',the acc is', 0.57085717, ', the cost is', 1.3098239)\n",
            "('the step is:', 295, ',the acc is', 0.6395714, ', the cost is', 1.175052)\n",
            "('the step is:', 300, ',the acc is', 0.65671426, ', the cost is', 1.1208081)\n",
            "('the step is:', 305, ',the acc is', 0.64114285, ', the cost is', 1.1569303)\n",
            "('the step is:', 310, ',the acc is', 0.62685716, ', the cost is', 1.1601442)\n",
            "('the step is:', 315, ',the acc is', 0.62785715, ', the cost is', 1.1875428)\n",
            "('the step is:', 320, ',the acc is', 0.65757143, ', the cost is', 1.1160998)\n",
            "('the step is:', 325, ',the acc is', 0.6297143, ', the cost is', 1.1501298)\n",
            "('the step is:', 330, ',the acc is', 0.6075714, ', the cost is', 1.2576014)\n",
            "('the step is:', 335, ',the acc is', 0.64185715, ', the cost is', 1.1703305)\n",
            "('the step is:', 340, ',the acc is', 0.6618571, ', the cost is', 1.1133454)\n",
            "('the step is:', 345, ',the acc is', 0.6391429, ', the cost is', 1.1480911)\n",
            "('the step is:', 350, ',the acc is', 0.6194286, ', the cost is', 1.2040246)\n",
            "('the step is:', 355, ',the acc is', 0.6495714, ', the cost is', 1.1356041)\n",
            "('the step is:', 360, ',the acc is', 0.6527143, ', the cost is', 1.118496)\n",
            "('the step is:', 365, ',the acc is', 0.569, ', the cost is', 1.334542)\n",
            "('the step is:', 370, ',the acc is', 0.6252857, ', the cost is', 1.1922884)\n",
            "('the step is:', 375, ',the acc is', 0.645, ', the cost is', 1.1385813)\n",
            "('the step is:', 380, ',the acc is', 0.6594286, ', the cost is', 1.112951)\n",
            "('the step is:', 385, ',the acc is', 0.5922857, ', the cost is', 1.2573271)\n",
            "('the step is:', 390, ',the acc is', 0.64657146, ', the cost is', 1.1473405)\n",
            "('the step is:', 395, ',the acc is', 0.6545714, ', the cost is', 1.1123903)\n",
            "('the step is:', 400, ',the acc is', 0.6175714, ', the cost is', 1.1834961)\n",
            "('the step is:', 405, ',the acc is', 0.61914283, ', the cost is', 1.2035562)\n",
            "('the step is:', 410, ',the acc is', 0.6277143, ', the cost is', 1.1868159)\n",
            "('the step is:', 415, ',the acc is', 0.65757143, ', the cost is', 1.1142294)\n",
            "('the step is:', 420, ',the acc is', 0.6275714, ', the cost is', 1.1705489)\n",
            "('the step is:', 425, ',the acc is', 0.62628573, ', the cost is', 1.1819198)\n",
            "('the step is:', 430, ',the acc is', 0.66157144, ', the cost is', 1.1079416)\n",
            "('the step is:', 435, ',the acc is', 0.61242855, ', the cost is', 1.1990652)\n",
            "('the step is:', 440, ',the acc is', 0.66014284, ', the cost is', 1.1099639)\n",
            "('the step is:', 445, ',the acc is', 0.6712857, ', the cost is', 1.083477)\n",
            "('the step is:', 450, ',the acc is', 0.67242855, ', the cost is', 1.0724199)\n",
            "('the step is:', 455, ',the acc is', 0.63757145, ', the cost is', 1.1720245)\n",
            "('the step is:', 460, ',the acc is', 0.5997143, ', the cost is', 1.2716137)\n",
            "('the step is:', 465, ',the acc is', 0.65585715, ', the cost is', 1.1472788)\n",
            "('the step is:', 470, ',the acc is', 0.6718571, ', the cost is', 1.1114371)\n",
            "('the step is:', 475, ',the acc is', 0.645, ', the cost is', 1.1334015)\n",
            "('the step is:', 480, ',the acc is', 0.6684286, ', the cost is', 1.1027131)\n",
            "('the step is:', 485, ',the acc is', 0.41685715, ', the cost is', 1.6235353)\n",
            "('the step is:', 490, ',the acc is', 0.568, ', the cost is', 1.3547024)\n",
            "('the step is:', 495, ',the acc is', 0.6094286, ', the cost is', 1.2472651)\n",
            "('the step is:', 500, ',the acc is', 0.6217143, ', the cost is', 1.2266005)\n",
            "('the step is:', 505, ',the acc is', 0.6312857, ', the cost is', 1.1879554)\n",
            "('the step is:', 510, ',the acc is', 0.6275714, ', the cost is', 1.1813239)\n",
            "('the step is:', 515, ',the acc is', 0.6312857, ', the cost is', 1.1903372)\n",
            "('the step is:', 520, ',the acc is', 0.607, ', the cost is', 1.203152)\n",
            "('the step is:', 525, ',the acc is', 0.6512857, ', the cost is', 1.1347868)\n",
            "('the step is:', 530, ',the acc is', 0.577, ', the cost is', 1.298672)\n",
            "('the step is:', 535, ',the acc is', 0.66071427, ', the cost is', 1.1269418)\n",
            "('the step is:', 540, ',the acc is', 0.593, ', the cost is', 1.279048)\n",
            "('the step is:', 545, ',the acc is', 0.64942855, ', the cost is', 1.1481788)\n",
            "('the step is:', 550, ',the acc is', 0.65885717, ', the cost is', 1.1108236)\n",
            "('the step is:', 555, ',the acc is', 0.628, ', the cost is', 1.1890503)\n",
            "('the step is:', 560, ',the acc is', 0.6652857, ', the cost is', 1.113204)\n",
            "('the step is:', 565, ',the acc is', 0.63014287, ', the cost is', 1.211546)\n",
            "('the step is:', 570, ',the acc is', 0.5972857, ', the cost is', 1.2312975)\n",
            "('the step is:', 575, ',the acc is', 0.65614283, ', the cost is', 1.1344517)\n",
            "('the step is:', 580, ',the acc is', 0.66714287, ', the cost is', 1.0983083)\n",
            "('the step is:', 585, ',the acc is', 0.6347143, ', the cost is', 1.1531764)\n",
            "('the step is:', 590, ',the acc is', 0.6481429, ', the cost is', 1.1425631)\n",
            "('the step is:', 595, ',the acc is', 0.6232857, ', the cost is', 1.2070342)\n",
            "('the step is:', 600, ',the acc is', 0.6734286, ', the cost is', 1.1003428)\n",
            "('the step is:', 605, ',the acc is', 0.67714286, ', the cost is', 1.084273)\n",
            "('the step is:', 610, ',the acc is', 0.582, ', the cost is', 1.3133339)\n",
            "('the step is:', 615, ',the acc is', 0.645, ', the cost is', 1.1471685)\n",
            "('the step is:', 620, ',the acc is', 0.5882857, ', the cost is', 1.2528491)\n",
            "('the step is:', 625, ',the acc is', 0.67071426, ', the cost is', 1.1002226)\n",
            "('the step is:', 630, ',the acc is', 0.64942855, ', the cost is', 1.126231)\n",
            "('the step is:', 635, ',the acc is', 0.66071427, ', the cost is', 1.107968)\n",
            "('the step is:', 640, ',the acc is', 0.64085716, ', the cost is', 1.1584616)\n",
            "('the step is:', 645, ',the acc is', 0.6438571, ', the cost is', 1.1468925)\n",
            "('the step is:', 650, ',the acc is', 0.6661429, ', the cost is', 1.0992322)\n",
            "('the step is:', 655, ',the acc is', 0.6761429, ', the cost is', 1.0799567)\n",
            "('the step is:', 660, ',the acc is', 0.608, ', the cost is', 1.249127)\n",
            "('the step is:', 665, ',the acc is', 0.6611429, ', the cost is', 1.1248134)\n",
            "('the step is:', 670, ',the acc is', 0.6827143, ', the cost is', 1.0739123)\n",
            "('the step is:', 675, ',the acc is', 0.6312857, ', the cost is', 1.1647074)\n",
            "('the step is:', 680, ',the acc is', 0.58614284, ', the cost is', 1.291242)\n",
            "('the step is:', 685, ',the acc is', 0.6495714, ', the cost is', 1.1565623)\n",
            "('the step is:', 690, ',the acc is', 0.67414284, ', the cost is', 1.0921366)\n",
            "('the step is:', 695, ',the acc is', 0.6882857, ', the cost is', 1.0628933)\n",
            "('the step is:', 700, ',the acc is', 0.6292857, ', the cost is', 1.1616106)\n",
            "('the step is:', 705, ',the acc is', 0.6547143, ', the cost is', 1.1193495)\n",
            "('the step is:', 710, ',the acc is', 0.6595714, ', the cost is', 1.1038482)\n",
            "('the step is:', 715, ',the acc is', 0.62357146, ', the cost is', 1.1856647)\n",
            "('the step is:', 720, ',the acc is', 0.6711429, ', the cost is', 1.1036648)\n",
            "('the step is:', 725, ',the acc is', 0.677, ', the cost is', 1.0678236)\n",
            "('the step is:', 730, ',the acc is', 0.6735714, ', the cost is', 1.0852542)\n",
            "('the step is:', 735, ',the acc is', 0.6248571, ', the cost is', 1.2030869)\n",
            "('the step is:', 740, ',the acc is', 0.6555714, ', the cost is', 1.1316988)\n",
            "('the step is:', 745, ',the acc is', 0.69014287, ', the cost is', 1.0656813)\n",
            "('the step is:', 750, ',the acc is', 0.658, ', the cost is', 1.1186961)\n",
            "('the step is:', 755, ',the acc is', 0.61557144, ', the cost is', 1.1988713)\n",
            "('the step is:', 760, ',the acc is', 0.6775714, ', the cost is', 1.0839823)\n",
            "('the step is:', 765, ',the acc is', 0.68385714, ', the cost is', 1.0629202)\n",
            "('the step is:', 770, ',the acc is', 0.65114284, ', the cost is', 1.13889)\n",
            "('the step is:', 775, ',the acc is', 0.67142856, ', the cost is', 1.1023972)\n",
            "('the step is:', 780, ',the acc is', 0.6837143, ', the cost is', 1.0683548)\n",
            "('the step is:', 785, ',the acc is', 0.6925714, ', the cost is', 1.0419619)\n",
            "('the step is:', 790, ',the acc is', 0.6244286, ', the cost is', 1.2218032)\n",
            "('the step is:', 795, ',the acc is', 0.68457144, ', the cost is', 1.0716039)\n",
            "('the step is:', 800, ',the acc is', 0.70271426, ', the cost is', 1.0291083)\n",
            "('the step is:', 805, ',the acc is', 0.66328573, ', the cost is', 1.1277066)\n",
            "('the step is:', 810, ',the acc is', 0.68642855, ', the cost is', 1.0817043)\n",
            "('the step is:', 815, ',the acc is', 0.66585714, ', the cost is', 1.1066029)\n",
            "('the step is:', 820, ',the acc is', 0.62214285, ', the cost is', 1.2045687)\n",
            "('the step is:', 825, ',the acc is', 0.67, ', the cost is', 1.1141803)\n",
            "('the step is:', 830, ',the acc is', 0.6704286, ', the cost is', 1.10862)\n",
            "('the step is:', 835, ',the acc is', 0.6395714, ', the cost is', 1.1659158)\n",
            "('the step is:', 840, ',the acc is', 0.65085715, ', the cost is', 1.1429616)\n",
            "('the step is:', 845, ',the acc is', 0.6801429, ', the cost is', 1.0774326)\n",
            "('the step is:', 850, ',the acc is', 0.60071427, ', the cost is', 1.2310259)\n",
            "('the step is:', 855, ',the acc is', 0.6891429, ', the cost is', 1.0725843)\n",
            "('the step is:', 860, ',the acc is', 0.665, ', the cost is', 1.106267)\n",
            "('the step is:', 865, ',the acc is', 0.6184286, ', the cost is', 1.2289424)\n",
            "('the step is:', 870, ',the acc is', 0.6717143, ', the cost is', 1.1040292)\n",
            "('the step is:', 875, ',the acc is', 0.6637143, ', the cost is', 1.112331)\n",
            "('the step is:', 880, ',the acc is', 0.6037143, ', the cost is', 1.2190105)\n",
            "('the step is:', 885, ',the acc is', 0.669, ', the cost is', 1.1093612)\n",
            "('the step is:', 890, ',the acc is', 0.6787143, ', the cost is', 1.0818741)\n",
            "('the step is:', 895, ',the acc is', 0.69314283, ', the cost is', 1.0592954)\n",
            "('the step is:', 900, ',the acc is', 0.6305714, ', the cost is', 1.18285)\n",
            "('the step is:', 905, ',the acc is', 0.64614284, ', the cost is', 1.1637652)\n",
            "('the step is:', 910, ',the acc is', 0.68628573, ', the cost is', 1.0730233)\n",
            "('the step is:', 915, ',the acc is', 0.64014286, ', the cost is', 1.1663578)\n",
            "('the step is:', 920, ',the acc is', 0.664, ', the cost is', 1.1096407)\n",
            "('the step is:', 925, ',the acc is', 0.6564286, ', the cost is', 1.1215255)\n",
            "('the step is:', 930, ',the acc is', 0.58, ', the cost is', 1.2953504)\n",
            "('the step is:', 935, ',the acc is', 0.671, ', the cost is', 1.1100001)\n",
            "('the step is:', 940, ',the acc is', 0.68885714, ', the cost is', 1.0589259)\n",
            "('the step is:', 945, ',the acc is', 0.6704286, ', the cost is', 1.1125679)\n",
            "('the step is:', 950, ',the acc is', 0.5795714, ', the cost is', 1.3335251)\n",
            "('the step is:', 955, ',the acc is', 0.6291429, ', the cost is', 1.1934037)\n",
            "('the step is:', 960, ',the acc is', 0.6668571, ', the cost is', 1.1087191)\n",
            "('the step is:', 965, ',the acc is', 0.62685716, ', the cost is', 1.1708403)\n",
            "('the step is:', 970, ',the acc is', 0.6727143, ', the cost is', 1.0763628)\n",
            "('the step is:', 975, ',the acc is', 0.6865714, ', the cost is', 1.054465)\n",
            "('the step is:', 980, ',the acc is', 0.6547143, ', the cost is', 1.1268344)\n",
            "('the step is:', 985, ',the acc is', 0.6112857, ', the cost is', 1.2317773)\n",
            "('the step is:', 990, ',the acc is', 0.6682857, ', the cost is', 1.1192515)\n",
            "('the step is:', 995, ',the acc is', 0.68471426, ', the cost is', 1.0737333)\n",
            "('the step is:', 1000, ',the acc is', 0.5665714, ', the cost is', 1.2984402)\n",
            "('the step is:', 1005, ',the acc is', 0.61957145, ', the cost is', 1.2525499)\n",
            "('the step is:', 1010, ',the acc is', 0.668, ', the cost is', 1.1297867)\n",
            "('the step is:', 1015, ',the acc is', 0.66585714, ', the cost is', 1.1145664)\n",
            "('the step is:', 1020, ',the acc is', 0.6858571, ', the cost is', 1.0790026)\n",
            "('the step is:', 1025, ',the acc is', 0.63185716, ', the cost is', 1.1960533)\n",
            "('the step is:', 1030, ',the acc is', 0.645, ', the cost is', 1.1460229)\n",
            "('the step is:', 1035, ',the acc is', 0.65985715, ', the cost is', 1.1459295)\n",
            "('the step is:', 1040, ',the acc is', 0.67214286, ', the cost is', 1.0939498)\n",
            "('the step is:', 1045, ',the acc is', 0.5672857, ', the cost is', 1.3303945)\n",
            "('the step is:', 1050, ',the acc is', 0.646, ', the cost is', 1.1674328)\n",
            "('the step is:', 1055, ',the acc is', 0.66242856, ', the cost is', 1.1107259)\n",
            "('the step is:', 1060, ',the acc is', 0.6787143, ', the cost is', 1.0862103)\n",
            "('the step is:', 1065, ',the acc is', 0.6342857, ', the cost is', 1.1853437)\n",
            "('the step is:', 1070, ',the acc is', 0.66885716, ', the cost is', 1.1036823)\n",
            "('the step is:', 1075, ',the acc is', 0.66885716, ', the cost is', 1.1015319)\n",
            "('the step is:', 1080, ',the acc is', 0.66328573, ', the cost is', 1.0984313)\n",
            "('the step is:', 1085, ',the acc is', 0.68114287, ', the cost is', 1.0763397)\n",
            "('the step is:', 1090, ',the acc is', 0.65657145, ', the cost is', 1.1376365)\n",
            "('the step is:', 1095, ',the acc is', 0.64114285, ', the cost is', 1.170137)\n",
            "('the step is:', 1100, ',the acc is', 0.6621429, ', the cost is', 1.1139561)\n",
            "('the step is:', 1105, ',the acc is', 0.679, ', the cost is', 1.087108)\n",
            "('the step is:', 1110, ',the acc is', 0.7014286, ', the cost is', 1.0397053)\n",
            "('the step is:', 1115, ',the acc is', 0.67628574, ', the cost is', 1.0880696)\n",
            "('the step is:', 1120, ',the acc is', 0.6932857, ', the cost is', 1.0621986)\n",
            "('the step is:', 1125, ',the acc is', 0.6661429, ', the cost is', 1.1056156)\n",
            "('the step is:', 1130, ',the acc is', 0.67742854, ', the cost is', 1.0845506)\n",
            "('the step is:', 1135, ',the acc is', 0.6537143, ', the cost is', 1.1351665)\n",
            "('the step is:', 1140, ',the acc is', 0.6587143, ', the cost is', 1.1316174)\n",
            "('the step is:', 1145, ',the acc is', 0.6682857, ', the cost is', 1.1454681)\n",
            "('the step is:', 1150, ',the acc is', 0.6872857, ', the cost is', 1.0832478)\n",
            "('the step is:', 1155, ',the acc is', 0.66785717, ', the cost is', 1.0975921)\n",
            "('the step is:', 1160, ',the acc is', 0.6075714, ', the cost is', 1.2687306)\n",
            "('the step is:', 1165, ',the acc is', 0.6857143, ', the cost is', 1.1005529)\n",
            "('the step is:', 1170, ',the acc is', 0.6897143, ', the cost is', 1.0625281)\n",
            "('the step is:', 1175, ',the acc is', 0.61642855, ', the cost is', 1.25331)\n",
            "('the step is:', 1180, ',the acc is', 0.6474286, ', the cost is', 1.1559824)\n",
            "('the step is:', 1185, ',the acc is', 0.66914284, ', the cost is', 1.0972136)\n",
            "('the step is:', 1190, ',the acc is', 0.62185717, ', the cost is', 1.184902)\n",
            "('the step is:', 1195, ',the acc is', 0.6742857, ', the cost is', 1.1121219)\n",
            "('the step is:', 1200, ',the acc is', 0.6947143, ', the cost is', 1.0459257)\n",
            "('the step is:', 1205, ',the acc is', 0.5964286, ', the cost is', 1.2407193)\n",
            "('the step is:', 1210, ',the acc is', 0.63928574, ', the cost is', 1.1928002)\n",
            "('the step is:', 1215, ',the acc is', 0.6837143, ', the cost is', 1.0783749)\n",
            "('the step is:', 1220, ',the acc is', 0.7031429, ', the cost is', 1.0327564)\n",
            "('the step is:', 1225, ',the acc is', 0.69771427, ', the cost is', 1.0453368)\n",
            "('the step is:', 1230, ',the acc is', 0.52485716, ', the cost is', 1.3937285)\n",
            "('the step is:', 1235, ',the acc is', 0.6578571, ', the cost is', 1.1368089)\n",
            "('the step is:', 1240, ',the acc is', 0.6474286, ', the cost is', 1.1751467)\n",
            "('the step is:', 1245, ',the acc is', 0.6874286, ', the cost is', 1.077795)\n",
            "('the step is:', 1250, ',the acc is', 0.7082857, ', the cost is', 1.0278372)\n",
            "('the step is:', 1255, ',the acc is', 0.58257145, ', the cost is', 1.3371644)\n",
            "('the step is:', 1260, ',the acc is', 0.64685714, ', the cost is', 1.1591803)\n",
            "('the step is:', 1265, ',the acc is', 0.6927143, ', the cost is', 1.0637467)\n",
            "('the step is:', 1270, ',the acc is', 0.6907143, ', the cost is', 1.0670133)\n",
            "('the step is:', 1275, ',the acc is', 0.6857143, ', the cost is', 1.0764742)\n",
            "('the step is:', 1280, ',the acc is', 0.6757143, ', the cost is', 1.1292034)\n",
            "('the step is:', 1285, ',the acc is', 0.6941429, ', the cost is', 1.069238)\n",
            "('the step is:', 1290, ',the acc is', 0.6991429, ', the cost is', 1.0529147)\n",
            "('the step is:', 1295, ',the acc is', 0.7017143, ', the cost is', 1.0469259)\n",
            "('the step is:', 1300, ',the acc is', 0.66342854, ', the cost is', 1.1324687)\n",
            "('the step is:', 1305, ',the acc is', 0.5954286, ', the cost is', 1.2924365)\n",
            "('the step is:', 1310, ',the acc is', 0.6802857, ', the cost is', 1.0992341)\n",
            "('the step is:', 1315, ',the acc is', 0.658, ', the cost is', 1.1167848)\n",
            "('the step is:', 1320, ',the acc is', 0.6692857, ', the cost is', 1.110252)\n",
            "('the step is:', 1325, ',the acc is', 0.69385713, ', the cost is', 1.049406)\n",
            "('the step is:', 1330, ',the acc is', 0.6298571, ', the cost is', 1.1980047)\n",
            "('the step is:', 1335, ',the acc is', 0.67714286, ', the cost is', 1.098896)\n",
            "('the step is:', 1340, ',the acc is', 0.696, ', the cost is', 1.0549351)\n",
            "('the step is:', 1345, ',the acc is', 0.6981429, ', the cost is', 1.0283229)\n",
            "('the step is:', 1350, ',the acc is', 0.5962857, ', the cost is', 1.271127)\n",
            "('the step is:', 1355, ',the acc is', 0.685, ', the cost is', 1.0944761)\n",
            "('the step is:', 1360, ',the acc is', 0.70842856, ', the cost is', 1.0354607)\n",
            "('the step is:', 1365, ',the acc is', 0.70857143, ', the cost is', 1.028696)\n",
            "('the step is:', 1370, ',the acc is', 0.5387143, ', the cost is', 1.388681)\n",
            "('the step is:', 1375, ',the acc is', 0.66742855, ', the cost is', 1.1347222)\n",
            "('the step is:', 1380, ',the acc is', 0.67914283, ', the cost is', 1.1030083)\n",
            "('the step is:', 1385, ',the acc is', 0.6858571, ', the cost is', 1.0746784)\n",
            "('the step is:', 1390, ',the acc is', 0.64771426, ', the cost is', 1.151836)\n",
            "('the step is:', 1395, ',the acc is', 0.65657145, ', the cost is', 1.1330323)\n",
            "('the step is:', 1400, ',the acc is', 0.6761429, ', the cost is', 1.1037068)\n",
            "('the step is:', 1405, ',the acc is', 0.69142854, ', the cost is', 1.0532026)\n",
            "('the step is:', 1410, ',the acc is', 0.6925714, ', the cost is', 1.0586482)\n",
            "('the step is:', 1415, ',the acc is', 0.634, ', the cost is', 1.1864644)\n",
            "('the step is:', 1420, ',the acc is', 0.6312857, ', the cost is', 1.1693137)\n",
            "('the step is:', 1425, ',the acc is', 0.682, ', the cost is', 1.0695308)\n",
            "('the step is:', 1430, ',the acc is', 0.67142856, ', the cost is', 1.0916034)\n",
            "('the step is:', 1435, ',the acc is', 0.6825714, ', the cost is', 1.065345)\n",
            "('the step is:', 1440, ',the acc is', 0.70085716, ', the cost is', 1.0364417)\n",
            "('the step is:', 1445, ',the acc is', 0.651, ', the cost is', 1.1320436)\n",
            "('the step is:', 1450, ',the acc is', 0.68128574, ', the cost is', 1.0774521)\n",
            "('the step is:', 1455, ',the acc is', 0.6452857, ', the cost is', 1.1530436)\n",
            "('the step is:', 1460, ',the acc is', 0.64085716, ', the cost is', 1.1594051)\n",
            "('the step is:', 1465, ',the acc is', 0.64342856, ', the cost is', 1.140274)\n",
            "('the step is:', 1470, ',the acc is', 0.67085713, ', the cost is', 1.100914)\n",
            "('the step is:', 1475, ',the acc is', 0.67457145, ', the cost is', 1.0864938)\n",
            "('the step is:', 1480, ',the acc is', 0.68614286, ', the cost is', 1.072745)\n",
            "('the step is:', 1485, ',the acc is', 0.6602857, ', the cost is', 1.1331655)\n",
            "('the step is:', 1490, ',the acc is', 0.6915714, ', the cost is', 1.0581036)\n",
            "('the step is:', 1495, ',the acc is', 0.6125714, ', the cost is', 1.2251488)\n",
            "('the shape of cnn output features', (28000, 64), (28000, 6))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aau_rLS9r1T0",
        "colab_type": "code",
        "outputId": "b1f4aaa9-3366-40bc-b981-3a17e113908b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1312
        }
      },
      "cell_type": "code",
      "source": [
        "#######RNN\n",
        "feature_all=feature_all\n",
        "no_fea=feature_all.shape[-1]\n",
        "print (no_fea)\n",
        "# The input to each LSTM layer must be a 3D\n",
        "# feature_all.reshape(samples-batch size-,time step, features)\n",
        "\n",
        "feature_all =feature_all.reshape([final,1,no_fea])\n",
        "#argmax returns the index with the largest value across axis of a tensor\n",
        "print (tf.argmax(label_all,1))\n",
        "\n",
        "\n",
        "print (label_all.shape)\n",
        "\n",
        "# middle_number=21000\n",
        "feature_training =feature_all[0:middle_number]\n",
        "feature_testing =feature_all[middle_number:final]\n",
        "label_training =label_all[0:middle_number]\n",
        "label_testing =label_all[middle_number:final]\n",
        "# print \"label_testing\",label_testing\n",
        "a=feature_training\n",
        "b=feature_testing\n",
        "print(feature_training.shape)\n",
        "print(feature_testing.shape)\n",
        "#264 dimention vector, that is passed to the next layer \n",
        "nodes=264\n",
        "#Used for Weight regulrization \n",
        "lameda=0.004\n",
        "#learning rate\n",
        "lr=0.005\n",
        "\n",
        "batch_size=final-middle_number\n",
        "train_fea=[]\n",
        "n_group=3\n",
        "for i in range(n_group):\n",
        "    f =a[(0+batch_size*i):(batch_size+batch_size*i)]\n",
        "    train_fea.append(f)\n",
        "  \n",
        "print(\"here\")\n",
        "print (train_fea[0].shape)\n",
        "\n",
        "train_label=[]\n",
        "for i in range(n_group):\n",
        "    f =label_training[(0+batch_size*i):(batch_size+batch_size*i), :]\n",
        "    train_label.append(f)\n",
        "print (train_label[0].shape)\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "n_inputs = no_fea\n",
        "n_steps = 1 # time steps\n",
        "n_hidden1_units = nodes   # neurons in hidden layer\n",
        "n_hidden2_units = nodes\n",
        "n_hidden3_units = nodes\n",
        "n_hidden4_units=nodes\n",
        "n_classes = n_classes\n",
        "\n",
        "# tf Graph input\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Define weights\n",
        "#tf.random_normal: Outputs random values from a normal distribution\n",
        "weights = {\n",
        "\n",
        "'in': tf.Variable(tf.random_normal([n_inputs, n_hidden1_units]), trainable=True),\n",
        "'a': tf.Variable(tf.random_normal([n_hidden1_units, n_hidden1_units]), trainable=True),\n",
        "\n",
        "'hidd2': tf.Variable(tf.random_normal([n_hidden1_units, n_hidden2_units])),\n",
        "'hidd3': tf.Variable(tf.random_normal([n_hidden2_units, n_hidden3_units])),\n",
        "'hidd4': tf.Variable(tf.random_normal([n_hidden3_units, n_hidden4_units])),\n",
        "\n",
        "'out': tf.Variable(tf.random_normal([n_hidden4_units, n_classes]), trainable=True),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "#tf.constant result a 1-D tensor of value 0.1\n",
        "'in': tf.Variable(tf.constant(0.1, shape=[n_hidden1_units])),\n",
        "\n",
        "'hidd2': tf.Variable(tf.constant(0.1, shape=[n_hidden2_units ])),\n",
        "'hidd3': tf.Variable(tf.constant(0.1, shape=[n_hidden3_units])),\n",
        "'hidd4': tf.Variable(tf.constant(0.1, shape=[n_hidden4_units])),\n",
        "\n",
        "'out': tf.Variable(tf.constant(0.1, shape=[n_classes ]), trainable=True)\n",
        "}\n",
        "\n",
        "\n",
        "def RNN(X, weights, biases):\n",
        "    # hidden layer for input to cell\n",
        "    ########################################\n",
        "\n",
        "    # transpose the inputs shape from\n",
        "    X = tf.reshape(X, [-1, n_inputs])\n",
        "\n",
        "    # into hidden\n",
        "    #there are n input and output we take only the last output to feed to the next layer\n",
        "    X_hidd1 = tf.matmul(X, weights['in']) + biases['in']\n",
        "    X_hidd2 = tf.matmul(X_hidd1, weights['hidd2']) + biases['hidd2']\n",
        "    X_hidd3 = tf.matmul(X_hidd2, weights['hidd3']) + biases['hidd3']\n",
        "    X_hidd4 = tf.matmul(X_hidd3, weights['hidd4']) + biases['hidd4']\n",
        "    X_in = tf.reshape(X_hidd4, [-1, n_steps, n_hidden4_units])\n",
        "\n",
        "\n",
        "    # cell\n",
        "    ##########################################\n",
        "\n",
        "    # basic LSTM Cell.\n",
        "    # 1-layer LSTM with n_hidden units.\n",
        "    # creates a LSTM layer and instantiates variables for all gates.\n",
        "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden4_units, forget_bias=1.0, state_is_tuple=True)\n",
        "    # 2nd layer LSTM with n_hidden units.\n",
        "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden4_units, forget_bias=1.0, state_is_tuple=True)\n",
        "    # Adding an additional layer to inprove the accuracy\n",
        "    # RNN cell composed sequentially of multiple simple cells.\n",
        "\n",
        "    lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
        "    # lstm cell is divided into two parts (c_state, h_state)\n",
        "    #Initializing the zero state\n",
        "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "    with tf.variable_scope('lstm1'):\n",
        "        # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
        "        outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
        "\n",
        "    # hidden layer for output as the final results\n",
        "    #############################################\n",
        "    print(\"before\")\n",
        "    print(outputs)\n",
        "    outputs = tf.unstack(tf.transpose(outputs, [1, 0, 2]))    # states is the last outputs\n",
        "    print(\"after\")\n",
        "    print(outputs)\n",
        "    #there are n input and n output we take only the last output to feed to the next layer\n",
        "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "    return results, outputs[-1]\n",
        "\n",
        "#################################################################################################################################################\n",
        "\n",
        "pred,Feature = RNN(x, weights, biases)\n",
        "lamena =lameda\n",
        "l2 = lamena * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())  # L2 loss prevents this overkill neural network to overfit the data\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) + l2  # Softmax loss\n",
        "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
        "    # train_op = tf.train.AdagradOptimizer(l).minimize(cost)\n",
        "    # train_op = tf.train.RMSPropOptimizer(0.00001).minimize(cost)\n",
        "    # train_op = tf.train.AdagradDAOptimizer(0.01).minimize(cost)\n",
        "    # train_op = tf.train.GradientDescentOptimizer(0.00001).minimize(cost)\n",
        "# pred_result =tf.argmax(pred, 1)\n",
        "label_true =tf.argmax(y, 1)\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "confusion_m=tf.confusion_matrix(tf.argmax(y, 1), tf.argmax(pred, 1))\n",
        "#starting sessions\n",
        "with tf.Session() as sess:\n",
        "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "        init = tf.initialize_all_variables()\n",
        "    else:\n",
        "        init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    saver = tf.train.Saver()\n",
        "    step = 0\n",
        "    \n",
        "    \n",
        "  \n",
        "    #downloaded = drive.CreateFile({'id':'10p_NuiBV2Or2sk6cm0yPLfu9tJ2lXEKg'})\n",
        "    #f2 = downloaded.GetContentString()  \n",
        "      \n",
        "    #filename = \"/home/xiangzhang/scratch/results/rnn_acc.csv\"\n",
        "    #f2 = open(filename, 'wb')\n",
        "    while step < 2500:\n",
        "        for i in range(n_group):\n",
        "            sess.run(train_op, feed_dict={\n",
        "                x: train_fea[i],\n",
        "                y: train_label[i],\n",
        "            })\n",
        "        if sess.run(accuracy, feed_dict={x: b,y: label_testing,})>0.96:\n",
        "            print(\n",
        "            \"The lamda is :\", lamena, \", Learning rate:\", lr, \", The step is:\", step, \", The accuracy is: \",\n",
        "            sess.run(accuracy, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            }))\n",
        "\n",
        "            break\n",
        "        if step % 5 == 0:\n",
        "            hh=sess.run(accuracy, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            })\n",
        "            #f2.write(str(hh)+'\\n')\n",
        "            print(\", The step is:\",step,\", The accuracy is:\", hh, \"The cost is :\",sess.run(cost, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            }))\n",
        "        step += 1\n",
        "\n",
        "    ##confusion matrix\n",
        "    feature_0=sess.run(Feature, feed_dict={x: train_fea[0]})\n",
        "    for i in range(1,n_group):\n",
        "        feature_11=sess.run(Feature, feed_dict={x: train_fea[i]})\n",
        "        feature_0=np.vstack((feature_0,feature_11))\n",
        "\n",
        "    print (feature_0.shape)\n",
        "    feature_b = sess.run(Feature, feed_dict={x: b})\n",
        "    feature_all_rnn=np.vstack((feature_0,feature_b))\n",
        "\n",
        "    confusion_m=sess.run(confusion_m, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            })\n",
        "    print (confusion_m)\n",
        "    ## predict probility\n",
        "    # pred_prob=sess.run(pred, feed_dict={\n",
        "    #             x: b,\n",
        "    #             y: label_testing,\n",
        "    #         })\n",
        "    # # print pred_prob\n",
        "\n",
        "\n",
        "    #print (\"RNN train time:\", time4 - time3, \"Rnn test time\", time5 - time4, 'RNN total time', time5 - time3)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "Tensor(\"ArgMax_600:0\", shape=(28000,), dtype=int64)\n",
            "(28000, 6)\n",
            "(21000, 1, 64)\n",
            "(7000, 1, 64)\n",
            "here\n",
            "(7000, 1, 64)\n",
            "(7000, 6)\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-9-ce823a85861c>:111: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-9-ce823a85861c>:117: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-9-ce823a85861c>:124: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "before\n",
            "Tensor(\"lstm1/rnn/transpose_1:0\", shape=(7000, 1, 264), dtype=float32)\n",
            "after\n",
            "[<tf.Tensor 'unstack:0' shape=(7000, 264) dtype=float32>]\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/confusion_matrix.py:194: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "(', The step is:', 0, ', The accuracy is:', 0.23014286, 'The cost is :', 587.56104)\n",
            "(', The step is:', 5, ', The accuracy is:', 0.30085716, 'The cost is :', 529.95154)\n",
            "(', The step is:', 10, ', The accuracy is:', 0.327, 'The cost is :', 476.42407)\n",
            "(', The step is:', 15, ', The accuracy is:', 0.34342858, 'The cost is :', 427.20367)\n",
            "(', The step is:', 20, ', The accuracy is:', 0.34957144, 'The cost is :', 382.58844)\n",
            "(', The step is:', 25, ', The accuracy is:', 0.354, 'The cost is :', 342.3905)\n",
            "(', The step is:', 30, ', The accuracy is:', 0.36257142, 'The cost is :', 306.2469)\n",
            "(', The step is:', 35, ', The accuracy is:', 0.35014287, 'The cost is :', 273.9254)\n",
            "(', The step is:', 40, ', The accuracy is:', 0.337, 'The cost is :', 245.03682)\n",
            "(', The step is:', 45, ', The accuracy is:', 0.33285713, 'The cost is :', 219.1664)\n",
            "(', The step is:', 50, ', The accuracy is:', 0.335, 'The cost is :', 195.9766)\n",
            "(', The step is:', 55, ', The accuracy is:', 0.336, 'The cost is :', 175.20372)\n",
            "(', The step is:', 60, ', The accuracy is:', 0.3392857, 'The cost is :', 156.61975)\n",
            "(', The step is:', 65, ', The accuracy is:', 0.34585714, 'The cost is :', 140.0178)\n",
            "(', The step is:', 70, ', The accuracy is:', 0.332, 'The cost is :', 125.199356)\n",
            "(', The step is:', 75, ', The accuracy is:', 0.34642857, 'The cost is :', 111.94322)\n",
            "(', The step is:', 80, ', The accuracy is:', 0.34685713, 'The cost is :', 100.11204)\n",
            "(', The step is:', 85, ', The accuracy is:', 0.35485715, 'The cost is :', 89.551605)\n",
            "(', The step is:', 90, ', The accuracy is:', 0.36985713, 'The cost is :', 80.12882)\n",
            "(', The step is:', 95, ', The accuracy is:', 0.37714285, 'The cost is :', 71.70652)\n",
            "(', The step is:', 100, ', The accuracy is:', 0.42785713, 'The cost is :', 64.145584)\n",
            "(', The step is:', 105, ', The accuracy is:', 0.4597143, 'The cost is :', 57.398376)\n",
            "(', The step is:', 110, ', The accuracy is:', 0.49785715, 'The cost is :', 51.358513)\n",
            "(', The step is:', 115, ', The accuracy is:', 0.5447143, 'The cost is :', 45.948368)\n",
            "(', The step is:', 120, ', The accuracy is:', 0.61457145, 'The cost is :', 41.046608)\n",
            "(', The step is:', 125, ', The accuracy is:', 0.65885717, 'The cost is :', 36.699165)\n",
            "(', The step is:', 130, ', The accuracy is:', 0.72914284, 'The cost is :', 32.769928)\n",
            "(', The step is:', 135, ', The accuracy is:', 0.7892857, 'The cost is :', 29.266685)\n",
            "(', The step is:', 140, ', The accuracy is:', 0.84185714, 'The cost is :', 26.132086)\n",
            "(', The step is:', 145, ', The accuracy is:', 0.88214284, 'The cost is :', 23.33985)\n",
            "(', The step is:', 150, ', The accuracy is:', 0.9244286, 'The cost is :', 20.829748)\n",
            "(', The step is:', 155, ', The accuracy is:', 0.943, 'The cost is :', 18.614971)\n",
            "(', The step is:', 160, ', The accuracy is:', 0.95614284, 'The cost is :', 16.645655)\n",
            "('The lamda is :', 0.004, ', Learning rate:', 0.005, ', The step is:', 164, ', The accuracy is: ', 0.9604286)\n",
            "(21000, 264)\n",
            "[[   0    0    0    0    0    0]\n",
            " [   0 2264   19   20   12   11]\n",
            " [   0   33 1169   13   13    9]\n",
            " [   0   18    9 1041   11   11]\n",
            " [   0   18    9    5 1041    9]\n",
            " [   0   33   14    7    3 1208]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A1ACrN-dr-O-",
        "colab_type": "code",
        "outputId": "a597de2c-ff01-4a16-cda0-98765b0cfa0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "cell_type": "code",
      "source": [
        "##AE\n",
        "print (feature_all_rnn.shape, feature_all_cnn.shape)\n",
        "# stacks the featurese from RNN and CNN in a horizontal stack \n",
        "feature_all=np.hstack((feature_all_rnn,psd))\n",
        "feature_all=np.hstack((feature_all,feature_all_cnn))\n",
        "print(psd.shape, feature_all.shape)\n",
        "no_fea=feature_all.shape[-1]\n",
        "\n",
        "# feature_all =feature_all.reshape([28000,1,no_fea])\n",
        "print (label_all.shape)\n",
        "\n",
        "# middle_number=21000\n",
        "feature_training =feature_all[0:middle_number]\n",
        "feature_testing =feature_all[middle_number:final]\n",
        "label_training =label_all[0:middle_number]\n",
        "label_testing =label_all[middle_number:final]\n",
        "# print \"label_testing\",label_testing\n",
        "a=feature_training\n",
        "b=feature_testing\n",
        "feature_all=feature_all\n",
        "print(feature_all.shape)\n",
        "\n",
        "train_fea=feature_all[0:middle_number]\n",
        "\n",
        "#dividing the input into three groups\n",
        "group=3\n",
        "display_step = 10\n",
        "#An epoch is a full iteration over samples!!!! training cycle \n",
        "training_epochs = 400\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 800 # 1st layer num features, should be times of 8\n",
        "\n",
        "\n",
        "n_hidden_2=100\n",
        "\n",
        "n_input_ae = no_fea # MNIST data input (img shape: 28*28)\n",
        "\n",
        "# tf Graph input (only pictures)\n",
        "X = tf.placeholder(\"float\", [None, n_input_ae])\n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.random_normal([n_input_ae, n_hidden_1])),\n",
        "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #NOT USED !!!\n",
        "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
        "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input_ae])),\n",
        "}\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'decoder_b2': tf.Variable(tf.random_normal([n_input_ae])),\n",
        "}\n",
        "\n",
        "\n",
        "# Building the encoder\n",
        "def encoder(x):\n",
        "    # Encoder Hidden layer with sigmoid activation #1\n",
        "    #Sigmoid function outputs in the range (0, 1), it makes it ideal for binary classification problems\n",
        "    #there are n input and output we take only the last output to feed to the next layer\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
        "                                   biases['encoder_b1']))\n",
        "    return layer_1\n",
        "\n",
        "\n",
        "# Building the decoder\n",
        "def decoder(x):\n",
        "    # Encoder Hidden layer with sigmoid activation #1\n",
        "    #Sigmoid function outputs in the range (0, 1), it makes it ideal for binary classification problems\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h2']),\n",
        "                                   biases['decoder_b2']))\n",
        "    return layer_1\n",
        "\n",
        "for ll in range(1):\n",
        "    learning_rate = 0.2\n",
        "    for ee in range(1):\n",
        "        # Construct model\n",
        "        encoder_op = encoder(X)\n",
        "        decoder_op = decoder(encoder_op)\n",
        "        # Prediction\n",
        "        y_pred = decoder_op\n",
        "        # Targets (Labels) are the input data, as the auto encoder tries to make output as similar as possible to the input.\n",
        "        y_true = X\n",
        "\n",
        "        # Define loss and optimizer, minimize the squared error\n",
        "        cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
        "        # cost = tf.reduce_mean(tf.pow(y_true, y_pred))\n",
        "        optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "        # Initializing the variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Launch the graph\n",
        "        # saves and restore variables\n",
        "        saver = tf.train.Saver()\n",
        "        with tf.Session() as sess1:\n",
        "            sess1.run(init)\n",
        "            saver = tf.train.Saver()\n",
        "            # Training cycle\n",
        "            for epoch in range(training_epochs):\n",
        "                # Loop over all batches\n",
        "                for i in range(group):\n",
        "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
        "                    _, c = sess1.run([optimizer, cost], feed_dict={X: a})\n",
        "                # Display logs per epoch step\n",
        "                if epoch % display_step == 0:\n",
        "                    print(\"Epoch:\", '%04d' % (epoch+1),\n",
        "                          \"cost=\", \"{:.9f}\".format(c))\n",
        "            print(\"Optimization Finished!\")\n",
        "            a = sess1.run(encoder_op, feed_dict={X: a})\n",
        "            b = sess1.run(encoder_op, feed_dict={X: b})\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((28000, 264), (28000, 120))\n",
            "((28000, 33), (28000, 417))\n",
            "(28000, 6)\n",
            "(28000, 417)\n",
            "('Epoch:', '0001', 'cost=', '0.451621443')\n",
            "('Epoch:', '0011', 'cost=', '0.439053148')\n",
            "('Epoch:', '0021', 'cost=', '0.381504685')\n",
            "('Epoch:', '0031', 'cost=', '0.218058035')\n",
            "('Epoch:', '0041', 'cost=', '0.107759520')\n",
            "('Epoch:', '0051', 'cost=', '0.079249017')\n",
            "('Epoch:', '0061', 'cost=', '0.070019484')\n",
            "('Epoch:', '0071', 'cost=', '0.062178422')\n",
            "('Epoch:', '0081', 'cost=', '0.046688385')\n",
            "('Epoch:', '0091', 'cost=', '0.046473116')\n",
            "('Epoch:', '0101', 'cost=', '0.046452876')\n",
            "('Epoch:', '0111', 'cost=', '0.047148481')\n",
            "('Epoch:', '0121', 'cost=', '0.047146805')\n",
            "('Epoch:', '0131', 'cost=', '0.047147546')\n",
            "('Epoch:', '0141', 'cost=', '0.047148447')\n",
            "('Epoch:', '0151', 'cost=', '0.047148731')\n",
            "('Epoch:', '0161', 'cost=', '0.047148932')\n",
            "('Epoch:', '0171', 'cost=', '0.047149189')\n",
            "('Epoch:', '0181', 'cost=', '0.047148328')\n",
            "('Epoch:', '0191', 'cost=', '0.044830751')\n",
            "('Epoch:', '0201', 'cost=', '0.044830859')\n",
            "('Epoch:', '0211', 'cost=', '0.044831049')\n",
            "('Epoch:', '0221', 'cost=', '0.044830751')\n",
            "('Epoch:', '0231', 'cost=', '0.044830061')\n",
            "('Epoch:', '0241', 'cost=', '0.044829514')\n",
            "('Epoch:', '0251', 'cost=', '0.044830691')\n",
            "('Epoch:', '0261', 'cost=', '0.044830613')\n",
            "('Epoch:', '0271', 'cost=', '0.044830419')\n",
            "('Epoch:', '0281', 'cost=', '0.044829708')\n",
            "('Epoch:', '0291', 'cost=', '0.044830129')\n",
            "('Epoch:', '0301', 'cost=', '0.044830881')\n",
            "('Epoch:', '0311', 'cost=', '0.044830766')\n",
            "('Epoch:', '0321', 'cost=', '0.044831108')\n",
            "('Epoch:', '0331', 'cost=', '0.044830542')\n",
            "('Epoch:', '0341', 'cost=', '0.044830751')\n",
            "('Epoch:', '0351', 'cost=', '0.044830501')\n",
            "('Epoch:', '0361', 'cost=', '0.044829842')\n",
            "('Epoch:', '0371', 'cost=', '0.044829838')\n",
            "('Epoch:', '0381', 'cost=', '0.042909943')\n",
            "('Epoch:', '0391', 'cost=', '0.042911194')\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GN_I3sj1sNcU",
        "colab_type": "code",
        "outputId": "5bb4d4b2-13cc-42bc-b44b-2f60b23a7422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8755
        }
      },
      "cell_type": "code",
      "source": [
        "##XGBoost\n",
        "import xgboost as xgb\n",
        "xg_train = xgb.DMatrix(a, label=np.argmax(label_training,1))\n",
        "xg_test = xgb.DMatrix(b, label=np.argmax(label_testing,1))\n",
        "\n",
        "# setup parameters for xgboost\n",
        "param = {}\n",
        "# use softmax multi-class classification\n",
        "param['objective'] = 'multi:softprob' # can I replace softmax by SVM??\n",
        "# softprob produce a matrix with probability value of each class\n",
        "# scale weight of positive examples\n",
        "param['eta'] = 0.5\n",
        "\n",
        "param['max_depth'] = 6\n",
        "param['silent'] = 1\n",
        "param['nthread'] = 4\n",
        "param['subsample']=0.9\n",
        "# param['lambda']=1\n",
        "param['num_class'] =n_classes\n",
        "\n",
        "\n",
        "\n",
        "#np.set_printoptions(threshold=np.nan)\n",
        "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
        "num_round = 500\n",
        "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
        "pred = bst.predict( xg_test );\n",
        "print(\"Prediction\")\n",
        "print(pred)\n",
        "#\n",
        "#print ('predicting, classification error=%f' % ((sum( int(pred[i]) != label_testing[i] for i in range(len(label_testing))) / float(len(label_testing)) )))\n",
        "\n",
        "\n",
        "# print (\"CNN train time:\", time2-time1, \"cnn test time\", time3-time2, 'CNN total time', time3-time1)\n",
        "# print (\"RNN train time:\", time4 - time3, \"Rnn test time\", time5 - time4, 'RNN total time', time5 - time3)\n",
        "# print (\"AE train time:\", time6 - time5, \"AE test time\", time7 - time6, 'AE total time', time7 - time5)\n",
        "# print (\"XGB train time:\", time8 - time7, \"XGB test time\", time9 - time8, 'XGB total time', time9 - time7)\n",
        "# print 'total train time', time2-time1+time4 - time3+time6 - time5+time8 - time7, 'total test time',time3-time2+time5 - time4+time7 - time6+time9 - time8, 'total run time', time9-time1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-merror:0.011857\ttest-merror:0.064571\n",
            "[1]\ttrain-merror:0.007714\ttest-merror:0.054286\n",
            "[2]\ttrain-merror:0.006619\ttest-merror:0.052571\n",
            "[3]\ttrain-merror:0.006095\ttest-merror:0.049571\n",
            "[4]\ttrain-merror:0.006\ttest-merror:0.046429\n",
            "[5]\ttrain-merror:0.005905\ttest-merror:0.045286\n",
            "[6]\ttrain-merror:0.005857\ttest-merror:0.045571\n",
            "[7]\ttrain-merror:0.005857\ttest-merror:0.043857\n",
            "[8]\ttrain-merror:0.005857\ttest-merror:0.043857\n",
            "[9]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[10]\ttrain-merror:0.005857\ttest-merror:0.043714\n",
            "[11]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[12]\ttrain-merror:0.005857\ttest-merror:0.044714\n",
            "[13]\ttrain-merror:0.005857\ttest-merror:0.044714\n",
            "[14]\ttrain-merror:0.005857\ttest-merror:0.044286\n",
            "[15]\ttrain-merror:0.005857\ttest-merror:0.044571\n",
            "[16]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[17]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[18]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[19]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[20]\ttrain-merror:0.005857\ttest-merror:0.042571\n",
            "[21]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[22]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[23]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[24]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[25]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[26]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[27]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[28]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[29]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[30]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[31]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[32]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[33]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[34]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[35]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[36]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[37]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[38]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[39]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[40]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[41]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[42]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[43]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[44]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[45]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[46]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[47]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[48]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[49]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[50]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[51]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[52]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[53]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[54]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[55]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[56]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[57]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[58]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[59]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[60]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[61]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[62]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[63]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[64]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[65]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[66]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[67]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[68]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[69]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[70]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[71]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[72]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[73]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[74]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[75]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[76]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[77]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[78]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[79]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[80]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[81]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[82]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[83]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[84]\ttrain-merror:0.005857\ttest-merror:0.042571\n",
            "[85]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[86]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[87]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[88]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[89]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[90]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[91]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[92]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[93]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[94]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[95]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[96]\ttrain-merror:0.005857\ttest-merror:0.042714\n",
            "[97]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[98]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[99]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[100]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[101]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[102]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[103]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[104]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[105]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[106]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[107]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[108]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[109]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[110]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[111]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[112]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[113]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[114]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[115]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[116]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[117]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[118]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[119]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[120]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[121]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[122]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[123]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[124]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[125]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[126]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[127]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[128]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[129]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[130]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[131]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[132]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[133]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[134]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[135]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[136]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[137]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[138]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[139]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[140]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[141]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[142]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[143]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[144]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[145]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[146]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[147]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[148]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[149]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[150]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[151]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[152]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[153]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[154]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[155]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[156]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[157]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[158]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[159]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[160]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[161]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[162]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[163]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[164]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[165]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[166]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[167]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[168]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[169]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[170]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[171]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[172]\ttrain-merror:0.005857\ttest-merror:0.042857\n",
            "[173]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[174]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[175]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[176]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[177]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[178]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[179]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[180]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[181]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[182]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[183]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[184]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[185]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[186]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[187]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[188]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[189]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[190]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[191]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[192]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[193]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[194]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[195]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[196]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[197]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[198]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[199]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[200]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[201]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[202]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[203]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "[204]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[205]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[206]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[207]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[208]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[209]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[210]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[211]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[212]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[213]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[214]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[215]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[216]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[217]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[218]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[219]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[220]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[221]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[222]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[223]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[224]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[225]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[226]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[227]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[228]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[229]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[230]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[231]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[232]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[233]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[234]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[235]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[236]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[237]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[238]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[239]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[240]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[241]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[242]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[243]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[244]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[245]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[246]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[247]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[248]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[249]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[250]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[251]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[252]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[253]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[254]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[255]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[256]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[257]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[258]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[259]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[260]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[261]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[262]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[263]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[264]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[265]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[266]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[267]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[268]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[269]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[270]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[271]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[272]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[273]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[274]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[275]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[276]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[277]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[278]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[279]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[280]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[281]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[282]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[283]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[284]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[285]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[286]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[287]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[288]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[289]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[290]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[291]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[292]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[293]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[294]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[295]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[296]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[297]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[298]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[299]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[300]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[301]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[302]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[303]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[304]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[305]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[306]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[307]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[308]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[309]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[310]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[311]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[312]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[313]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[314]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[315]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[316]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[317]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[318]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[319]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[320]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[321]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[322]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[323]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[324]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[325]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[326]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[327]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[328]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[329]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[330]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[331]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[332]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[333]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[334]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[335]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[336]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[337]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[338]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[339]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[340]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[341]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[342]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[343]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[344]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[345]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[346]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[347]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[348]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[349]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[350]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[351]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[352]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[353]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[354]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[355]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[356]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[357]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[358]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[359]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[360]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[361]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[362]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[363]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[364]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[365]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[366]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[367]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[368]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[369]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[370]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[371]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[372]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[373]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[374]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[375]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[376]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[377]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[378]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[379]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[380]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[381]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[382]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[383]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[384]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[385]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[386]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[387]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[388]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[389]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[390]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[391]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[392]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[393]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[394]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[395]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[396]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[397]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[398]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[399]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[400]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[401]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[402]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[403]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[404]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[405]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[406]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[407]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[408]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[409]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[410]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[411]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[412]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[413]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[414]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[415]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[416]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[417]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[418]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[419]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[420]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[421]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[422]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[423]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[424]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[425]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[426]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[427]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[428]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[429]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[430]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[431]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[432]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[433]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[434]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[435]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[436]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[437]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[438]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[439]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[440]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[441]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[442]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[443]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[444]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[445]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[446]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[447]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[448]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[449]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[450]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[451]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[452]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[453]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[454]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[455]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[456]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[457]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[458]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[459]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[460]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[461]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[462]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[463]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[464]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[465]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[466]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[467]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[468]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[469]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[470]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[471]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[472]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[473]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[474]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[475]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[476]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[477]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[478]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[479]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[480]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[481]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[482]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[483]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[484]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[485]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[486]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[487]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[488]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[489]\ttrain-merror:0.005857\ttest-merror:0.043571\n",
            "[490]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[491]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[492]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[493]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[494]\ttrain-merror:0.005857\ttest-merror:0.043429\n",
            "[495]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[496]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[497]\ttrain-merror:0.005857\ttest-merror:0.043143\n",
            "[498]\ttrain-merror:0.005857\ttest-merror:0.043286\n",
            "[499]\ttrain-merror:0.005857\ttest-merror:0.043\n",
            "Prediction\n",
            "[[9.4022097e-09 9.9999774e-01 1.0076955e-06 2.5069272e-07 8.3428472e-07\n",
            "  1.1841919e-07]\n",
            " [9.6625294e-09 9.4313052e-08 9.9999905e-01 2.9130197e-07 2.5554684e-07\n",
            "  2.8981066e-07]\n",
            " [1.1337647e-08 9.9999845e-01 9.0796505e-08 3.3485941e-07 8.8540628e-07\n",
            "  2.0820485e-07]\n",
            " ...\n",
            " [8.1225544e-08 4.7842044e-05 1.9001424e-06 9.9994504e-01 2.1481883e-06\n",
            "  2.9603211e-06]\n",
            " [4.9429286e-07 6.6363355e-06 6.0965262e-06 7.4605974e-05 1.3072662e-05\n",
            "  9.9989903e-01]\n",
            " [8.8623992e-08 4.6463563e-07 2.3640147e-03 5.1218026e-06 2.3438565e-06\n",
            "  9.9762803e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xm2avC2z7iv2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bst.save_model('0001.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AzxdsSe79RnG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dump model\n",
        "bst.dump_model('dump.raw.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgxV1-mr4sCb",
        "colab_type": "code",
        "outputId": "14dc67e1-2c6f-40c1-db8b-27f59dd6a2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print ('predicting, classification error=%f' % ((sum( int(pred[i][j]) != label_testing[i][j] \n",
        "                                                     for i in range((label_testing.shape[0]))\n",
        "                                                    for j in range (label_testing.shape[1])) / float(len(label_testing)) )))\n",
        "\n",
        "intent_labeling = np.array(['','eye_closed', 'left_hand', 'right_hand', 'both_hands', 'both_feet'])\n",
        "pred_argmax = np.argmax(pred,1)\n",
        "print(pred_argmax.shape)\n",
        "print(pred_argmax)\n",
        "copy_pred = np.empty(pred.shape, dtype=object)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicting, classification error=1.000000\n",
            "(7000,)\n",
            "[1 2 1 ... 3 5 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a4jqhUU4F4JE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range (pred.shape[0]):\n",
        "  copy_pred[i] = intent_labeling[pred_argmax[i]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lHlumyjOcnfn",
        "colab_type": "code",
        "outputId": "d1341305-b372-499c-8667-7246916c982e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "print(copy_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['eye_closed' 'eye_closed' 'eye_closed' 'eye_closed' 'eye_closed'\n",
            "  'eye_closed']\n",
            " ['left_hand' 'left_hand' 'left_hand' 'left_hand' 'left_hand' 'left_hand']\n",
            " ['eye_closed' 'eye_closed' 'eye_closed' 'eye_closed' 'eye_closed'\n",
            "  'eye_closed']\n",
            " ...\n",
            " ['right_hand' 'right_hand' 'right_hand' 'right_hand' 'right_hand'\n",
            "  'right_hand']\n",
            " ['both_feet' 'both_feet' 'both_feet' 'both_feet' 'both_feet' 'both_feet']\n",
            " ['both_feet' 'both_feet' 'both_feet' 'both_feet' 'both_feet' 'both_feet']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UBsBcTI1iCKg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt\n",
        "import pickle\n",
        "\n",
        "# save model to file\n",
        "pickle.dump(bst, open(\"pima.pickle.dat\", \"wb\"))\n",
        "\n",
        "#save model to drive\n",
        "model_file = drive.CreateFile({'title' : 'pima.pickle.dat'})\n",
        "model_file.SetContentFile('pima.pickle.dat')      \n",
        "model_file.Upload()\n",
        "#download to google drive   \n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QGmquIkZiHxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_obj = drive.CreateFile({'id': '1-cWjb6pYtXvt-Ai9T4_9elNUuqFybMXA'})\n",
        "file_obj.GetContentFile('pima.pickle.dat')\n",
        "\n",
        "# load model from file\n",
        "loaded_model = pickle.load(open(\"pima.pickle.dat\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eucN0Jq2iMX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = loaded_model.predict( xg_test );\n",
        "print(\"Prediction\")\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}