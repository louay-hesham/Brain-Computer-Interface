{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotiv local dataset model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb3rAg49Reog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkls5uowRir0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ada-A9jASVlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZgPDwuy6HOg",
        "colab_type": "code",
        "outputId": "9d50efd7-1dbb-4f70-86f8-32ba469cc4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#2. Get the files\n",
        "feature_downloaded = drive.CreateFile({'id':'1F4Xf1CZkwVyMsX3FjfH9WkGe5-p_uRw6'})\n",
        "feature_downloaded.GetContentFile('full_local_data.mat')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0625 16:58:02.629417 140708618037120 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWCCTrlVSZhJ",
        "colab_type": "code",
        "outputId": "486ddd4b-c867-4063-c6b6-d4f71718732f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import scipy.io as sc\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import files\n",
        "from scipy.fftpack import rfft\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# this function is used to transfer one column label to one hot label\n",
        "def one_hot(y_):\n",
        "    # Function to encode output labels from number indexes\n",
        "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
        "    print(\"one hot\")\n",
        "    print(y_.shape)\n",
        "    y_ = y_.reshape(len(y_))\n",
        "    n_values = np.max(y_) + 1\n",
        "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  Data loading\n",
        "feature = sc.loadmat(\"full_local_data.mat\")\n",
        "\n",
        "all = feature['Eddeny3a2lk']\n",
        "print('Feature')\n",
        "print(feature)\n",
        "\n",
        "print('shape')\n",
        "print (all.shape)\n",
        "print('shape')\n",
        "print (all.shape)\n",
        "np.random.shuffle(all)   # mix eeg_all\n",
        "# Get the 28000 samples of that subject\n",
        "final=len(all)\n",
        "print(final)\n",
        "all=all[0:final]\n",
        "temp = all[final-1]\n",
        "temp = np.reshape(temp, (1,15))\n",
        "print(temp.shape)\n",
        "all = np.append(all, temp, axis=0)\n",
        "\n",
        "\n",
        "# Get the features\n",
        "feature_all =all[:,0:14]\n",
        "\n",
        "\n",
        "# Get the label\n",
        "labels=all[:,14]\n",
        "\n",
        "# z-score\n",
        "\n",
        "print(\"Feature All\")\n",
        "print(feature_all)\n",
        "# transposed = [list(i) for i in zip(*feature_all)] ## Transpose so each sub-array is the data of a channel in time domain\n",
        "# fft_transposed = [rfft(np.array(i)) for i in transposed] ## Compute FFT (real values)\n",
        "# fft_samples = [list(np.round(i, 0)) for i in zip(*fft_transposed)] ## Transpose again so each channel is in a column instead of a row, rounds to the nearest unit while transposing\n",
        "# feature_all = fft_samples[0:]\n",
        "\n",
        "# feature_all = np.asarray(feature_all)\n",
        "no_fea=feature_all.shape[-1]\n",
        "labels = labels.astype(int)\n",
        "\n",
        "print(\"labels\")\n",
        "print(labels)\n",
        "labels_all=one_hot(labels)\n",
        "print(\"\")\n",
        "print (labels_all)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature\n",
            "{'__version__': '1.0', 'Eddeny3a2lk': array([[4.67912121e+03, 4.32109091e+03, 4.60236364e+03, ...,\n",
            "        4.34324242e+03, 4.59206061e+03, 0.00000000e+00],\n",
            "       [4.68169697e+03, 4.32006061e+03, 4.60030303e+03, ...,\n",
            "        4.33190909e+03, 4.58845454e+03, 0.00000000e+00],\n",
            "       [4.69457576e+03, 4.33242424e+03, 4.61318182e+03, ...,\n",
            "        4.35509091e+03, 4.60184848e+03, 0.00000000e+00],\n",
            "       ...,\n",
            "       [4.62090909e+03, 4.30769697e+03, 4.66727273e+03, ...,\n",
            "        4.38496970e+03, 4.46842424e+03, 4.00000000e+00],\n",
            "       [4.60442424e+03, 4.30821212e+03, 4.65593939e+03, ...,\n",
            "        4.38393939e+03, 4.44884848e+03, 4.00000000e+00],\n",
            "       [4.61163636e+03, 4.30512121e+03, 4.65593939e+03, ...,\n",
            "        4.38445455e+03, 4.45039394e+03, 4.00000000e+00]]), '__header__': 'MATLAB 5.0 MAT-file Platform: nt, Created on: Tue Jun 25 15:34:06 2019', '__globals__': []}\n",
            "shape\n",
            "(43831, 15)\n",
            "shape\n",
            "(43831, 15)\n",
            "43831\n",
            "(1, 15)\n",
            "Feature All\n",
            "[[4470.48484804 4327.78787836 4654.39393893 ... 4733.21212074\n",
            "  4334.99999957 4359.21212078]\n",
            " [4492.63636319 4371.06060562 4631.21212075 ... 4623.99999954\n",
            "  4325.72727229 4173.75757534]\n",
            " [4415.36363592 4495.72727228 4703.33333286 ... 4719.30302983\n",
            "  4356.12121169 4469.45454501]\n",
            " ...\n",
            " [4623.99999954 4266.48484806 4675.51515105 ... 4526.12121167\n",
            "  4150.06060565 4500.36363591]\n",
            " [4629.6666662  4331.39393896 4621.42424196 ... 4629.15151469\n",
            "  4301.51515109 4501.90909046]\n",
            " [4629.6666662  4331.39393896 4621.42424196 ... 4629.15151469\n",
            "  4301.51515109 4501.90909046]]\n",
            "labels\n",
            "[0 1 1 ... 1 3 3]\n",
            "one hot\n",
            "(43832,)\n",
            "\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcuCvBfagbUX",
        "colab_type": "code",
        "outputId": "4c03f50e-1c4d-4f8b-9267-8ce703c9b504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        " # Define window length (4 seconds)\n",
        "win = 0.5 * sf\n",
        "freqs, psd = signal.welch(data, sf, nperseg=win)\n",
        "print('freqs')\n",
        "print(freqs)\n",
        "print('psd')\n",
        "print(psd)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "freqs\n",
            "[ 0.          9.14285714 18.28571429 27.42857143 36.57142857 45.71428571\n",
            " 54.85714286 64.        ]\n",
            "psd\n",
            "[[9.12513522e-03 1.99992810e-02 5.27022739e-03 ... 5.01860510e-04\n",
            "  4.02842864e-03 1.97606150e-03]\n",
            " [2.39622300e-04 1.32385661e-05 6.73881098e-04 ... 3.64627142e-03\n",
            "  1.97857131e-03 5.16269424e-04]\n",
            " [1.05667611e-02 5.34455888e-02 2.19093856e-03 ... 5.25860030e-02\n",
            "  1.32842827e-02 3.92147907e-05]\n",
            " ...\n",
            " [1.81302827e-02 5.21844021e-02 3.64639227e-02 ... 2.03589145e-03\n",
            "  8.17413974e-05 4.23339524e-05]\n",
            " [2.65331346e-03 9.11223962e-03 9.48863088e-03 ... 5.98760032e-03\n",
            "  6.68353114e-03 4.35648301e-03]\n",
            " [2.65331346e-03 9.11223962e-03 9.48863088e-03 ... 5.98760032e-03\n",
            "  6.68353114e-03 4.35648301e-03]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/scipy/signal/spectral.py:1970: UserWarning: nperseg = 64 is greater than input length  = 14, using nperseg = 14\n",
            "  .format(nperseg, input_length))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6GcCHoLS-T5",
        "colab_type": "code",
        "outputId": "9dd04119-2847-4f61-e059-2303cdcf15ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import signal\n",
        "\n",
        "from scipy.fftpack import rfft\n",
        "import numpy as np\n",
        "\n",
        "sns.set(font_scale=1.2)\n",
        "print(\"before\")\n",
        "print(feature_all)\n",
        "feature_all = preprocessing.scale(feature_all)\n",
        "print(\"after\")\n",
        "\n",
        "print(feature_all)\n",
        "\n",
        "data = feature_all\n",
        "\n",
        "# Define sampling frequency and time vector\n",
        "sf = 128\n",
        "time = np.arange(data.shape[0]) / sf\n",
        "print('data')\n",
        "print(data.shape)\n",
        "print('time')\n",
        "print(time.shape)\n",
        "# Plot the signal\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
        "plt.plot(time, data, lw=1.5, color='k')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Voltage')\n",
        "plt.xlim([time.min(), time.max()])\n",
        "plt.title('EEG Data')\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before\n",
            "[[4470.48484804 4327.78787836 4654.39393893 ... 4733.21212074\n",
            "  4334.99999957 4359.21212078]\n",
            " [4492.63636319 4371.06060562 4631.21212075 ... 4623.99999954\n",
            "  4325.72727229 4173.75757534]\n",
            " [4415.36363592 4495.72727228 4703.33333286 ... 4719.30302983\n",
            "  4356.12121169 4469.45454501]\n",
            " ...\n",
            " [4623.99999954 4266.48484806 4675.51515105 ... 4526.12121167\n",
            "  4150.06060565 4500.36363591]\n",
            " [4629.6666662  4331.39393896 4621.42424196 ... 4629.15151469\n",
            "  4301.51515109 4501.90909046]\n",
            " [4629.6666662  4331.39393896 4621.42424196 ... 4629.15151469\n",
            "  4301.51515109 4501.90909046]]\n",
            "after\n",
            "[[-1.10264226 -0.62693888  0.2734524  ...  1.86255717  0.04111842\n",
            "  -0.24860759]\n",
            " [-0.93977829 -0.1242802  -0.10953284 ...  0.0652846  -0.04582777\n",
            "  -0.92870626]\n",
            " [-1.50790841  1.32385551  1.0819768  ...  1.63365925  0.23916253\n",
            "   0.15567329]\n",
            " ...\n",
            " [ 0.02604293 -1.33903867  0.62239451 ... -1.54547855 -1.69297508\n",
            "   0.26902307]\n",
            " [ 0.0677058  -0.58505065 -0.27123772 ...  0.15006161 -0.27285394\n",
            "   0.27469056]\n",
            " [ 0.0677058  -0.58505065 -0.27123772 ...  0.15006161 -0.27285394\n",
            "   0.27469056]]\n",
            "data\n",
            "(43832, 14)\n",
            "time\n",
            "(43832,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAukAAAEhCAYAAADClz5PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSToRERERkcT8P9mRnZxN4qHhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcXwAnEYrhMn",
        "colab_type": "code",
        "outputId": "677b68ac-ddd8-4338-9ebf-63f38b4def80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_classes=5\n",
        "###CNN code,\n",
        "feature_all=feature_all# the input data of CNN\n",
        "print (\"cnn input feature shape\", feature_all.shape)\n",
        "n_fea=feature_all.shape[-1]\n",
        "print(n_fea)\n",
        "# labels_all=one_hot(labels_all)\n",
        "\n",
        "final=all.shape[0]\n",
        "middle_number=final*3/4\n",
        "print(\"-----\",middle_number)\n",
        "feature_training =feature_all[0:middle_number]\n",
        "feature_testing =feature_all[middle_number:final]\n",
        "label_training =labels_all[0:middle_number]\n",
        "label_testing =labels_all[middle_number:final]\n",
        "label_ww=labels_all[middle_number:final]  # for the confusion matrix\n",
        "print (\"label_testing\",label_testing.shape)\n",
        "a=feature_training\n",
        "b=feature_testing\n",
        "print(feature_training.shape)\n",
        "print(feature_testing.shape)\n",
        "\n",
        "print(\"Input shape\")\n",
        "print(b.shape)\n",
        "print(\"Label shape\")\n",
        "print(label_testing.shape)\n",
        "\n",
        "keep=1\n",
        "batch_size=final-middle_number\n",
        "n_group=3\n",
        "train_fea=[]\n",
        "for i in range(n_group):\n",
        "    f =a[(0+batch_size*i):(batch_size+batch_size*i)]\n",
        "    train_fea.append(f)\n",
        "print(\"Here\")\n",
        "print (train_fea[0].shape)\n",
        "\n",
        "train_label=[]\n",
        "for i in range(n_group):\n",
        "    f =label_training[(0+batch_size*i):(batch_size+batch_size*i), :]\n",
        "    train_label.append(f)\n",
        "print (train_label[0].shape)\n",
        "\n",
        "# the CNN code\n",
        "def compute_accuracy(v_xs, v_ys):\n",
        "    global prediction\n",
        "    y_pre = sess3.run(prediction, feed_dict={xs: v_xs, keep_prob: keep})\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    result = sess3.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: keep})\n",
        "    return result\n",
        "\n",
        "#to creat a random weights \n",
        "def weight_variable(shape):\n",
        "    # Outputs random values from a truncated normal distribution\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    # A variable maintains state in the graph across calls to run(). \n",
        "    # You add a variable to the graph by constructing an instance of the class Variable.\n",
        "    print('shape')\n",
        "    print(shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "#random bias values\n",
        "def bias_variable(shape):\n",
        "    # Creates a constant tensor\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    # stride [1, x_movement, y_movement, 1]\n",
        "    # Must have strides[0] = strides[3] = 1\n",
        "    # the concolution layer x is the input\n",
        "    # w is the weight and the stride is how many moves it makes in each dimention ie how many pixels\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "# def max_pool_2x2(x):\n",
        "#     # stride [1, x_movement, y_movement, 1]\n",
        "#     return tf.nn.max_pool(x, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "#max pooling to reduce dimentionality .. here consider every 1*2 window\n",
        "def max_pool_1x2(x):\n",
        "    # stride [1, x_movement, y_movement, 1]\n",
        "    return tf.nn.max_pool(x, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "\n",
        "# define placeholder for inputs to network\n",
        "xs = tf.placeholder(tf.float32, [None, n_fea]) # 1*14\n",
        "ys = tf.placeholder(tf.float32, [None, n_classes])  # 2 is the classes of the data\n",
        "# Lookup what is keep_prob\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "x_image = tf.reshape(xs, [-1, 1, n_fea, 1])\n",
        "print('xs')\n",
        "print(xs)\n",
        "print(xs.shape)\n",
        "print('x_image')\n",
        "print(x_image)\n",
        "print(x_image.shape)  \n",
        "\n",
        "## conv1 layer ##\n",
        "W_conv1 = weight_variable([1,1, 1,20]) # patch 1*1, in size is 1, out size is 2\n",
        "b_conv1 = bias_variable([20])\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 1*64*2\n",
        "h_pool1 = max_pool_1x2(h_conv1)                          # output size 1*32x2\n",
        "\n",
        "## conv2 layer ##\n",
        "# W_conv2 = weight_variable([1,1, 2, 4]) # patch 1*1, in size 2, out size 4\n",
        "# b_conv2 = bias_variable([4])\n",
        "# h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 1*32*4\n",
        "# h_pool2 = max_pool_1x2(h_conv2)                          # output size 1*16*4\n",
        "\n",
        "## fc1 layer ## fc fully connected layer\n",
        "W_fc1 = weight_variable([1*(n_fea/2)*20, 120])\n",
        "b_fc1 = bias_variable([120])\n",
        "# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]\n",
        "h_pool2_flat = tf.reshape(h_pool1, [-1, 1*(n_fea/2)*20])\n",
        "h_fc1 = tf.nn.sigmoid(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "## fc2 layer ##\n",
        "W_fc2 = weight_variable([120, n_classes])\n",
        "b_fc2 = bias_variable([n_classes])\n",
        "# Multiplies matrix a by matrix b, producing a * b\n",
        "prediction = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
        "\n",
        "# Weight regulrization\n",
        "l2 = 0.001 * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "# Getting the mean of the errors between the predication results and the class labels in the trainning data\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=ys))+l2   # Softmax loss\n",
        "# Using optimizer\n",
        "train_step = tf.train.AdamOptimizer(0.04).minimize(cross_entropy)\n",
        "# Begin session to visit the nodes (tensors) of the graph\n",
        "sess3 = tf.Session()\n",
        "# Initializae all the defined variables\n",
        "init = tf.global_variables_initializer()\n",
        "# Visit the nodes of those variables\n",
        "sess3.run(init)\n",
        "# Total number of array elements which trigger summarization rather than full array\n",
        "#np.set_printoptions(threshold=np.nan)\n",
        "step = 1\n",
        "while step < 1500:\n",
        "    # Train the model\n",
        "    for i in range(n_group):\n",
        "        sess3.run(train_step, feed_dict={xs: train_fea[i], ys: train_label[i], keep_prob:keep})\n",
        "    # After 5 steps, use the model on the test data\n",
        "    if step % 5 == 0:\n",
        "        # Compute the cost using the cross entropy\n",
        "        cost=sess3.run(cross_entropy, feed_dict={xs: b, ys: label_testing, keep_prob: keep})\n",
        "        # Compute the accuracy\n",
        "        acc_cnn_t=compute_accuracy(b, label_testing)\n",
        "        print('the step is:',step,',the acc is',acc_cnn_t,', the cost is', cost)\n",
        "    step+=1\n",
        "acc_cnn=compute_accuracy(b, label_testing)\n",
        "feature_all_cnn=sess3.run(h_fc1_drop, feed_dict={xs: feature_all, keep_prob: keep})\n",
        "print (\"the shape of cnn output features\",feature_all.shape,labels_all.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('cnn input feature shape', (43832, 14))\n",
            "14\n",
            "('-----', 32874)\n",
            "('label_testing', (10958, 5))\n",
            "(32874, 14)\n",
            "(10958, 14)\n",
            "Input shape\n",
            "(10958, 14)\n",
            "Label shape\n",
            "(10958, 5)\n",
            "Here\n",
            "(10958, 14)\n",
            "(10958, 5)\n",
            "xs\n",
            "Tensor(\"Placeholder_3:0\", shape=(?, 14), dtype=float32)\n",
            "(?, 14)\n",
            "x_image\n",
            "Tensor(\"Reshape_2:0\", shape=(?, 1, 14, 1), dtype=float32)\n",
            "(?, 1, 14, 1)\n",
            "shape\n",
            "[1, 1, 1, 20]\n",
            "shape\n",
            "[140, 120]\n",
            "shape\n",
            "[120, 5]\n",
            "('the step is:', 5, ',the acc is', 0.30854172, ', the cost is', 194.5906)\n",
            "('the step is:', 10, ',the acc is', 0.37059683, ', the cost is', 66.84056)\n",
            "('the step is:', 15, ',the acc is', 0.43329075, ', the cost is', 23.427603)\n",
            "('the step is:', 20, ',the acc is', 0.4589341, ', the cost is', 8.870596)\n",
            "('the step is:', 25, ',the acc is', 0.51186347, ', the cost is', 3.8979497)\n",
            "('the step is:', 30, ',the acc is', 0.54398614, ', the cost is', 2.1696463)\n",
            "('the step is:', 35, ',the acc is', 0.5748312, ', the cost is', 1.5645065)\n",
            "('the step is:', 40, ',the acc is', 0.6282168, ', the cost is', 1.2448361)\n",
            "('the step is:', 45, ',the acc is', 0.61297685, ', the cost is', 1.1991447)\n",
            "('the step is:', 50, ',the acc is', 0.67548823, ', the cost is', 1.0725927)\n",
            "('the step is:', 55, ',the acc is', 0.63971525, ', the cost is', 1.1160822)\n",
            "('the step is:', 60, ',the acc is', 0.66791385, ', the cost is', 1.0785437)\n",
            "('the step is:', 65, ',the acc is', 0.715094, ', the cost is', 0.982305)\n",
            "('the step is:', 70, ',the acc is', 0.73407555, ', the cost is', 0.938803)\n",
            "('the step is:', 75, ',the acc is', 0.68224126, ', the cost is', 1.067706)\n",
            "('the step is:', 80, ',the acc is', 0.74119365, ', the cost is', 0.9489312)\n",
            "('the step is:', 85, ',the acc is', 0.7132688, ', the cost is', 0.9875965)\n",
            "('the step is:', 90, ',the acc is', 0.7501369, ', the cost is', 0.9268156)\n",
            "('the step is:', 95, ',the acc is', 0.77194744, ', the cost is', 0.8855024)\n",
            "('the step is:', 100, ',the acc is', 0.7855448, ', the cost is', 0.8580079)\n",
            "('the step is:', 105, ',the acc is', 0.79923344, ', the cost is', 0.83517176)\n",
            "('the step is:', 110, ',the acc is', 0.7761453, ', the cost is', 0.87964064)\n",
            "('the step is:', 115, ',the acc is', 0.71582407, ', the cost is', 1.001163)\n",
            "('the step is:', 120, ',the acc is', 0.7955831, ', the cost is', 0.86049336)\n",
            "('the step is:', 125, ',the acc is', 0.78700495, ', the cost is', 0.86500984)\n",
            "('the step is:', 130, ',the acc is', 0.8027925, ', the cost is', 0.83947694)\n",
            "('the step is:', 135, ',the acc is', 0.7716737, ', the cost is', 0.8844941)\n",
            "('the step is:', 140, ',the acc is', 0.8130133, ', the cost is', 0.8349644)\n",
            "('the step is:', 145, ',the acc is', 0.8371966, ', the cost is', 0.80198145)\n",
            "('the step is:', 150, ',the acc is', 0.8375616, ', the cost is', 0.7876024)\n",
            "('the step is:', 155, ',the acc is', 0.77231246, ', the cost is', 0.8827542)\n",
            "('the step is:', 160, ',the acc is', 0.7939405, ', the cost is', 0.8670291)\n",
            "('the step is:', 165, ',the acc is', 0.7946706, ', the cost is', 0.8728651)\n",
            "('the step is:', 170, ',the acc is', 0.84696114, ', the cost is', 0.7883527)\n",
            "('the step is:', 175, ',the acc is', 0.8211352, ', the cost is', 0.8221004)\n",
            "('the step is:', 180, ',the acc is', 0.6649936, ', the cost is', 1.1336336)\n",
            "('the step is:', 185, ',the acc is', 0.78800875, ', the cost is', 0.9176844)\n",
            "('the step is:', 190, ',the acc is', 0.8459573, ', the cost is', 0.8082477)\n",
            "('the step is:', 195, ',the acc is', 0.85590434, ', the cost is', 0.776039)\n",
            "('the step is:', 200, ',the acc is', 0.85444427, ', the cost is', 0.7647613)\n",
            "('the step is:', 205, ',the acc is', 0.86010224, ', the cost is', 0.7540126)\n",
            "('the step is:', 210, ',the acc is', 0.7839934, ', the cost is', 0.90093005)\n",
            "('the step is:', 215, ',the acc is', 0.8399343, ', the cost is', 0.8216607)\n",
            "('the step is:', 220, ',the acc is', 0.8513415, ', the cost is', 0.78822887)\n",
            "('the step is:', 225, ',the acc is', 0.85745573, ', the cost is', 0.7697246)\n",
            "('the step is:', 230, ',the acc is', 0.8389305, ', the cost is', 0.78732157)\n",
            "('the step is:', 235, ',the acc is', 0.8465961, ', the cost is', 0.7728783)\n",
            "('the step is:', 240, ',the acc is', 0.82241285, ', the cost is', 0.81724143)\n",
            "('the step is:', 245, ',the acc is', 0.81785, ', the cost is', 0.82635117)\n",
            "('the step is:', 250, ',the acc is', 0.82305163, ', the cost is', 0.8347361)\n",
            "('the step is:', 255, ',the acc is', 0.86147106, ', the cost is', 0.7647628)\n",
            "('the step is:', 260, ',the acc is', 0.85617816, ', the cost is', 0.7665788)\n",
            "('the step is:', 265, ',the acc is', 0.8505202, ', the cost is', 0.7688129)\n",
            "('the step is:', 270, ',the acc is', 0.8521628, ', the cost is', 0.77514386)\n",
            "('the step is:', 275, ',the acc is', 0.7300602, ', the cost is', 1.0260313)\n",
            "('the step is:', 280, ',the acc is', 0.8449535, ', the cost is', 0.8165395)\n",
            "('the step is:', 285, ',the acc is', 0.866034, ', the cost is', 0.7717556)\n",
            "('the step is:', 290, ',the acc is', 0.8662165, ', the cost is', 0.75670344)\n",
            "('the step is:', 295, ',the acc is', 0.86192733, ', the cost is', 0.75230026)\n",
            "('the step is:', 300, ',the acc is', 0.8350064, ', the cost is', 0.80781573)\n",
            "('the step is:', 305, ',the acc is', 0.8695017, ', the cost is', 0.7537866)\n",
            "('the step is:', 310, ',the acc is', 0.8654864, ', the cost is', 0.75350255)\n",
            "('the step is:', 315, ',the acc is', 0.8517065, ', the cost is', 0.774413)\n",
            "('the step is:', 320, ',the acc is', 0.8679504, ', the cost is', 0.74941504)\n",
            "('the step is:', 325, ',the acc is', 0.8617448, ', the cost is', 0.7520441)\n",
            "('the step is:', 330, ',the acc is', 0.76765835, ', the cost is', 0.9299021)\n",
            "('the step is:', 335, ',the acc is', 0.8581858, ', the cost is', 0.7870839)\n",
            "('the step is:', 340, ',the acc is', 0.8705056, ', the cost is', 0.74701273)\n",
            "('the step is:', 345, ',the acc is', 0.8748859, ', the cost is', 0.73679805)\n",
            "('the step is:', 350, ',the acc is', 0.8572732, ', the cost is', 0.7624075)\n",
            "('the step is:', 355, ',the acc is', 0.7975908, ', the cost is', 0.87944585)\n",
            "('the step is:', 360, ',the acc is', 0.86576015, ', the cost is', 0.76298887)\n",
            "('the step is:', 365, ',the acc is', 0.8696842, ', the cost is', 0.74574065)\n",
            "('the step is:', 370, ',the acc is', 0.8753422, ', the cost is', 0.73139393)\n",
            "('the step is:', 375, ',the acc is', 0.80616903, ', the cost is', 0.85128)\n",
            "('the step is:', 380, ',the acc is', 0.8048914, ', the cost is', 0.8996313)\n",
            "('the step is:', 385, ',the acc is', 0.85371417, ', the cost is', 0.79092836)\n",
            "('the step is:', 390, ',the acc is', 0.8704143, ', the cost is', 0.75006914)\n",
            "('the step is:', 395, ',the acc is', 0.8757985, ', the cost is', 0.73394656)\n",
            "('the step is:', 400, ',the acc is', 0.87771493, ', the cost is', 0.7253556)\n",
            "('the step is:', 405, ',the acc is', 0.8805439, ', the cost is', 0.7214807)\n",
            "('the step is:', 410, ',the acc is', 0.77988684, ', the cost is', 0.8968974)\n",
            "('the step is:', 415, ',the acc is', 0.84303707, ', the cost is', 0.8074025)\n",
            "('the step is:', 420, ',the acc is', 0.87598103, ', the cost is', 0.7468145)\n",
            "('the step is:', 425, ',the acc is', 0.87570727, ', the cost is', 0.7372939)\n",
            "('the step is:', 430, ',the acc is', 0.8704143, ', the cost is', 0.74269843)\n",
            "('the step is:', 435, ',the acc is', 0.663351, ', the cost is', 1.152349)\n",
            "('the step is:', 440, ',the acc is', 0.86128855, ', the cost is', 0.78372806)\n",
            "('the step is:', 445, ',the acc is', 0.8643001, ', the cost is', 0.7661207)\n",
            "('the step is:', 450, ',the acc is', 0.8743384, ', the cost is', 0.7475233)\n",
            "('the step is:', 455, ',the acc is', 0.8740646, ', the cost is', 0.73835194)\n",
            "('the step is:', 460, ',the acc is', 0.87716734, ', the cost is', 0.73348033)\n",
            "('the step is:', 465, ',the acc is', 0.87442964, ', the cost is', 0.7290418)\n",
            "('the step is:', 470, ',the acc is', 0.8349151, ', the cost is', 0.80239)\n",
            "('the step is:', 475, ',the acc is', 0.8397518, ', the cost is', 0.7922903)\n",
            "('the step is:', 480, ',the acc is', 0.7999635, ', the cost is', 0.8716854)\n",
            "('the step is:', 485, ',the acc is', 0.8665815, ', the cost is', 0.7561477)\n",
            "('the step is:', 490, ',the acc is', 0.8704143, ', the cost is', 0.74331146)\n",
            "('the step is:', 495, ',the acc is', 0.85608685, ', the cost is', 0.76871115)\n",
            "('the step is:', 500, ',the acc is', 0.8646651, ', the cost is', 0.76153564)\n",
            "('the step is:', 505, ',the acc is', 0.875616, ', the cost is', 0.7385526)\n",
            "('the step is:', 510, ',the acc is', 0.8835554, ', the cost is', 0.7238431)\n",
            "('the step is:', 515, ',the acc is', 0.8706881, ', the cost is', 0.73640835)\n",
            "('the step is:', 520, ',the acc is', 0.7733163, ', the cost is', 0.9183446)\n",
            "('the step is:', 525, ',the acc is', 0.8335463, ', the cost is', 0.82490146)\n",
            "('the step is:', 530, ',the acc is', 0.86904544, ', the cost is', 0.7612212)\n",
            "('the step is:', 535, ',the acc is', 0.84504473, ', the cost is', 0.789366)\n",
            "('the step is:', 540, ',the acc is', 0.87734985, ', the cost is', 0.7374417)\n",
            "('the step is:', 545, ',the acc is', 0.8778062, ', the cost is', 0.7328633)\n",
            "('the step is:', 550, ',the acc is', 0.87287825, ', the cost is', 0.73795706)\n",
            "('the step is:', 555, ',the acc is', 0.8252418, ', the cost is', 0.8163497)\n",
            "('the step is:', 560, ',the acc is', 0.8486038, ', the cost is', 0.77994823)\n",
            "('the step is:', 565, ',the acc is', 0.8753422, ', the cost is', 0.74100614)\n",
            "('the step is:', 570, ',the acc is', 0.8708706, ', the cost is', 0.74317944)\n",
            "('the step is:', 575, ',the acc is', 0.82104397, ', the cost is', 0.8327091)\n",
            "('the step is:', 580, ',the acc is', 0.8385654, ', the cost is', 0.81084764)\n",
            "('the step is:', 585, ',the acc is', 0.8824603, ', the cost is', 0.735929)\n",
            "('the step is:', 590, ',the acc is', 0.8833729, ', the cost is', 0.72591907)\n",
            "('the step is:', 595, ',the acc is', 0.88601935, ', the cost is', 0.71801996)\n",
            "('the step is:', 600, ',the acc is', 0.85991967, ', the cost is', 0.7497432)\n",
            "('the step is:', 605, ',the acc is', 0.8757985, ', the cost is', 0.73567533)\n",
            "('the step is:', 610, ',the acc is', 0.7389122, ', the cost is', 1.0048418)\n",
            "('the step is:', 615, ',the acc is', 0.86283994, ', the cost is', 0.7862717)\n",
            "('the step is:', 620, ',the acc is', 0.88236904, ', the cost is', 0.73783416)\n",
            "('the step is:', 625, ',the acc is', 0.88538057, ', the cost is', 0.72306275)\n",
            "('the step is:', 630, ',the acc is', 0.8845592, ', the cost is', 0.7165977)\n",
            "('the step is:', 635, ',the acc is', 0.8828253, ', the cost is', 0.7150223)\n",
            "('the step is:', 640, ',the acc is', 0.851524, ', the cost is', 0.7679429)\n",
            "('the step is:', 645, ',the acc is', 0.88793576, ', the cost is', 0.71431255)\n",
            "('the step is:', 650, ',the acc is', 0.8802701, ', the cost is', 0.7217437)\n",
            "('the step is:', 655, ',the acc is', 0.6707428, ', the cost is', 1.19526)\n",
            "('the step is:', 660, ',the acc is', 0.8530754, ', the cost is', 0.8407787)\n",
            "('the step is:', 665, ',the acc is', 0.86977553, ', the cost is', 0.78233564)\n",
            "('the step is:', 670, ',the acc is', 0.8811827, ', the cost is', 0.7470027)\n",
            "('the step is:', 675, ',the acc is', 0.87598103, ', the cost is', 0.7357461)\n",
            "('the step is:', 680, ',the acc is', 0.8795401, ', the cost is', 0.7210137)\n",
            "('the step is:', 685, ',the acc is', 0.8807264, ', the cost is', 0.71678126)\n",
            "('the step is:', 690, ',the acc is', 0.88154775, ', the cost is', 0.712024)\n",
            "('the step is:', 695, ',the acc is', 0.8805439, ', the cost is', 0.7180476)\n",
            "('the step is:', 700, ',the acc is', 0.87963134, ', the cost is', 0.72074884)\n",
            "('the step is:', 705, ',the acc is', 0.8812739, ', the cost is', 0.7150756)\n",
            "('the step is:', 710, ',the acc is', 0.875616, ', the cost is', 0.7247661)\n",
            "('the step is:', 715, ',the acc is', 0.8830078, ', the cost is', 0.7152703)\n",
            "('the step is:', 720, ',the acc is', 0.85800326, ', the cost is', 0.7659384)\n",
            "('the step is:', 725, ',the acc is', 0.8021537, ', the cost is', 0.8741698)\n",
            "('the step is:', 730, ',the acc is', 0.86804163, ', the cost is', 0.75207615)\n",
            "('the step is:', 735, ',the acc is', 0.86731154, ', the cost is', 0.7536448)\n",
            "('the step is:', 740, ',the acc is', 0.8344588, ', the cost is', 0.81607246)\n",
            "('the step is:', 745, ',the acc is', 0.85800326, ', the cost is', 0.7644503)\n",
            "('the step is:', 750, ',the acc is', 0.8740646, ', the cost is', 0.74040806)\n",
            "('the step is:', 755, ',the acc is', 0.8670378, ', the cost is', 0.7464858)\n",
            "('the step is:', 760, ',the acc is', 0.8432196, ', the cost is', 0.7859168)\n",
            "('the step is:', 765, ',the acc is', 0.84851253, ', the cost is', 0.78751576)\n",
            "('the step is:', 770, ',the acc is', 0.87744117, ', the cost is', 0.73677945)\n",
            "('the step is:', 775, ',the acc is', 0.8893959, ', the cost is', 0.71168625)\n",
            "('the step is:', 780, ',the acc is', 0.87734985, ', the cost is', 0.7289644)\n",
            "('the step is:', 785, ',the acc is', 0.8636612, ', the cost is', 0.75248206)\n",
            "('the step is:', 790, ',the acc is', 0.7777879, ', the cost is', 0.9381285)\n",
            "('the step is:', 795, ',the acc is', 0.8653951, ', the cost is', 0.7653363)\n",
            "('the step is:', 800, ',the acc is', 0.8895784, ', the cost is', 0.72592044)\n",
            "('the step is:', 805, ',the acc is', 0.8913123, ', the cost is', 0.7170477)\n",
            "('the step is:', 810, ',the acc is', 0.8911298, ', the cost is', 0.7066705)\n",
            "('the step is:', 815, ',the acc is', 0.8923161, ', the cost is', 0.7024649)\n",
            "('the step is:', 820, ',the acc is', 0.8646651, ', the cost is', 0.74357355)\n",
            "('the step is:', 825, ',the acc is', 0.8924986, ', the cost is', 0.7066263)\n",
            "('the step is:', 830, ',the acc is', 0.8917686, ', the cost is', 0.70684063)\n",
            "('the step is:', 835, ',the acc is', 0.86630774, ', the cost is', 0.7466628)\n",
            "('the step is:', 840, ',the acc is', 0.87379086, ', the cost is', 0.73983216)\n",
            "('the step is:', 845, ',the acc is', 0.87999636, ', the cost is', 0.7258127)\n",
            "('the step is:', 850, ',the acc is', 0.88410294, ', the cost is', 0.71787703)\n",
            "('the step is:', 855, ',the acc is', 0.89605767, ', the cost is', 0.70032406)\n",
            "('the step is:', 860, ',the acc is', 0.8830078, ', the cost is', 0.719509)\n",
            "('the step is:', 865, ',the acc is', 0.89478004, ', the cost is', 0.6996237)\n",
            "('the step is:', 870, ',the acc is', 0.86576015, ', the cost is', 0.7469621)\n",
            "('the step is:', 875, ',the acc is', 0.89487135, ', the cost is', 0.7042698)\n",
            "('the step is:', 880, ',the acc is', 0.8789925, ', the cost is', 0.7200593)\n",
            "('the step is:', 885, ',the acc is', 0.8679504, ', the cost is', 0.75069904)\n",
            "('the step is:', 890, ',the acc is', 0.8175762, ', the cost is', 0.855582)\n",
            "('the step is:', 895, ',the acc is', 0.85891587, ', the cost is', 0.77257156)\n",
            "('the step is:', 900, ',the acc is', 0.8905822, ', the cost is', 0.7169094)\n",
            "('the step is:', 905, ',the acc is', 0.89258987, ', the cost is', 0.70846903)\n",
            "('the step is:', 910, ',the acc is', 0.89140356, ', the cost is', 0.7065759)\n",
            "('the step is:', 915, ',the acc is', 0.8797226, ', the cost is', 0.72251374)\n",
            "('the step is:', 920, ',the acc is', 0.88328165, ', the cost is', 0.7197537)\n",
            "('the step is:', 925, ',the acc is', 0.8949626, ', the cost is', 0.70042527)\n",
            "('the step is:', 930, ',the acc is', 0.89085597, ', the cost is', 0.7093212)\n",
            "('the step is:', 935, ',the acc is', 0.8632962, ', the cost is', 0.7561835)\n",
            "('the step is:', 940, ',the acc is', 0.78527105, ', the cost is', 0.9047234)\n",
            "('the step is:', 945, ',the acc is', 0.88255155, ', the cost is', 0.7305834)\n",
            "('the step is:', 950, ',the acc is', 0.8890309, ', the cost is', 0.71811295)\n",
            "('the step is:', 955, ',the acc is', 0.89405, ', the cost is', 0.7062223)\n",
            "('the step is:', 960, ',the acc is', 0.89569265, ', the cost is', 0.6989778)\n",
            "('the step is:', 965, ',the acc is', 0.8573645, ', the cost is', 0.75605476)\n",
            "('the step is:', 970, ',the acc is', 0.8884833, ', the cost is', 0.7099152)\n",
            "('the step is:', 975, ',the acc is', 0.88538057, ', the cost is', 0.7151358)\n",
            "('the step is:', 980, ',the acc is', 0.88346416, ', the cost is', 0.7183721)\n",
            "('the step is:', 985, ',the acc is', 0.87908375, ', the cost is', 0.71961087)\n",
            "('the step is:', 990, ',the acc is', 0.87251323, ', the cost is', 0.73249346)\n",
            "('the step is:', 995, ',the acc is', 0.8679504, ', the cost is', 0.7523957)\n",
            "('the step is:', 1000, ',the acc is', 0.88601935, ', the cost is', 0.7198098)\n",
            "('the step is:', 1005, ',the acc is', 0.85526556, ', the cost is', 0.7643198)\n",
            "('the step is:', 1010, ',the acc is', 0.6963862, ', the cost is', 1.1098464)\n",
            "('the step is:', 1015, ',the acc is', 0.8745209, ', the cost is', 0.7761775)\n",
            "('the step is:', 1020, ',the acc is', 0.89067346, ', the cost is', 0.7342328)\n",
            "('the step is:', 1025, ',the acc is', 0.89030844, ', the cost is', 0.7198835)\n",
            "('the step is:', 1030, ',the acc is', 0.88811827, ', the cost is', 0.71329343)\n",
            "('the step is:', 1035, ',the acc is', 0.8914948, ', the cost is', 0.7060567)\n",
            "('the step is:', 1040, ',the acc is', 0.8951451, ', the cost is', 0.6988708)\n",
            "('the step is:', 1045, ',the acc is', 0.8926811, ', the cost is', 0.7035566)\n",
            "('the step is:', 1050, ',the acc is', 0.8951451, ', the cost is', 0.69988453)\n",
            "('the step is:', 1055, ',the acc is', 0.89569265, ', the cost is', 0.6995567)\n",
            "('the step is:', 1060, ',the acc is', 0.8942325, ', the cost is', 0.7033888)\n",
            "('the step is:', 1065, ',the acc is', 0.866034, ', the cost is', 0.747074)\n",
            "('the step is:', 1070, ',the acc is', 0.89158607, ', the cost is', 0.70709765)\n",
            "('the step is:', 1075, ',the acc is', 0.8610148, ', the cost is', 0.75943696)\n",
            "('the step is:', 1080, ',the acc is', 0.7277788, ', the cost is', 1.0411565)\n",
            "('the step is:', 1085, ',the acc is', 0.8401168, ', the cost is', 0.83156925)\n",
            "('the step is:', 1090, ',the acc is', 0.88428545, ', the cost is', 0.7433327)\n",
            "('the step is:', 1095, ',the acc is', 0.8899434, ', the cost is', 0.7254578)\n",
            "('the step is:', 1100, ',the acc is', 0.887297, ', the cost is', 0.7157155)\n",
            "('the step is:', 1105, ',the acc is', 0.8907647, ', the cost is', 0.7077595)\n",
            "('the step is:', 1110, ',the acc is', 0.8957839, ', the cost is', 0.69986415)\n",
            "('the step is:', 1115, ',the acc is', 0.8800876, ', the cost is', 0.7239457)\n",
            "('the step is:', 1120, ',the acc is', 0.8892134, ', the cost is', 0.7101635)\n",
            "('the step is:', 1125, ',the acc is', 0.89103854, ', the cost is', 0.70479834)\n",
            "('the step is:', 1130, ',the acc is', 0.8662165, ', the cost is', 0.74160635)\n",
            "('the step is:', 1135, ',the acc is', 0.8902172, ', the cost is', 0.7078049)\n",
            "('the step is:', 1140, ',the acc is', 0.89779156, ', the cost is', 0.6961986)\n",
            "('the step is:', 1145, ',the acc is', 0.85353166, ', the cost is', 0.7698449)\n",
            "('the step is:', 1150, ',the acc is', 0.88893956, ', the cost is', 0.71247804)\n",
            "('the step is:', 1155, ',the acc is', 0.8952364, ', the cost is', 0.69885886)\n",
            "('the step is:', 1160, ',the acc is', 0.8564519, ', the cost is', 0.76495415)\n",
            "('the step is:', 1165, ',the acc is', 0.8971528, ', the cost is', 0.7018975)\n",
            "('the step is:', 1170, ',the acc is', 0.89678776, ', the cost is', 0.69985676)\n",
            "('the step is:', 1175, ',the acc is', 0.86858916, ', the cost is', 0.74168384)\n",
            "('the step is:', 1180, ',the acc is', 0.84020805, ', the cost is', 0.79748905)\n",
            "('the step is:', 1185, ',the acc is', 0.7043256, ', the cost is', 1.1053864)\n",
            "('the step is:', 1190, ',the acc is', 0.8683154, ', the cost is', 0.78696275)\n",
            "('the step is:', 1195, ',the acc is', 0.88291657, ', the cost is', 0.7484628)\n",
            "('the step is:', 1200, ',the acc is', 0.8888483, ', the cost is', 0.7232481)\n",
            "('the step is:', 1205, ',the acc is', 0.87871873, ', the cost is', 0.7242479)\n",
            "('the step is:', 1210, ',the acc is', 0.8758898, ', the cost is', 0.72550225)\n",
            "('the step is:', 1215, ',the acc is', 0.8705968, ', the cost is', 0.7412487)\n",
            "('the step is:', 1220, ',the acc is', 0.89012593, ', the cost is', 0.71225494)\n",
            "('the step is:', 1225, ',the acc is', 0.88291657, ', the cost is', 0.71895266)\n",
            "('the step is:', 1230, ',the acc is', 0.86959296, ', the cost is', 0.73909307)\n",
            "('the step is:', 1235, ',the acc is', 0.8927724, ', the cost is', 0.70867866)\n",
            "('the step is:', 1240, ',the acc is', 0.8847417, ', the cost is', 0.71512675)\n",
            "('the step is:', 1245, ',the acc is', 0.88893956, ', the cost is', 0.7092706)\n",
            "('the step is:', 1250, ',the acc is', 0.8928637, ', the cost is', 0.7043173)\n",
            "('the step is:', 1255, ',the acc is', 0.82177407, ', the cost is', 0.8251931)\n",
            "('the step is:', 1260, ',the acc is', 0.82861835, ', the cost is', 0.8333616)\n",
            "('the step is:', 1265, ',the acc is', 0.88465047, ', the cost is', 0.73556936)\n",
            "('the step is:', 1270, ',the acc is', 0.8811827, ', the cost is', 0.7242143)\n",
            "('the step is:', 1275, ',the acc is', 0.887662, ', the cost is', 0.71575874)\n",
            "('the step is:', 1280, ',the acc is', 0.8627487, ', the cost is', 0.7548654)\n",
            "('the step is:', 1285, ',the acc is', 0.88583684, ', the cost is', 0.7127843)\n",
            "('the step is:', 1290, ',the acc is', 0.88857454, ', the cost is', 0.71349347)\n",
            "('the step is:', 1295, ',the acc is', 0.8776237, ', the cost is', 0.72814894)\n",
            "('the step is:', 1300, ',the acc is', 0.84331083, ', the cost is', 0.7884035)\n",
            "('the step is:', 1305, ',the acc is', 0.8662165, ', the cost is', 0.7473911)\n",
            "('the step is:', 1310, ',the acc is', 0.87442964, ', the cost is', 0.7369357)\n",
            "('the step is:', 1315, ',the acc is', 0.88255155, ', the cost is', 0.7204499)\n",
            "('the step is:', 1320, ',the acc is', 0.87881, ', the cost is', 0.71995056)\n",
            "('the step is:', 1325, ',the acc is', 0.8914948, ', the cost is', 0.7068347)\n",
            "('the step is:', 1330, ',the acc is', 0.8712356, ', the cost is', 0.73446107)\n",
            "('the step is:', 1335, ',the acc is', 0.8963314, ', the cost is', 0.7002952)\n",
            "('the step is:', 1340, ',the acc is', 0.8938675, ', the cost is', 0.70334566)\n",
            "('the step is:', 1345, ',the acc is', 0.87771493, ', the cost is', 0.7262783)\n",
            "('the step is:', 1350, ',the acc is', 0.8499726, ', the cost is', 0.7844185)\n",
            "('the step is:', 1355, ',the acc is', 0.79357547, ', the cost is', 0.91208255)\n",
            "('the step is:', 1360, ',the acc is', 0.8751597, ', the cost is', 0.75860554)\n",
            "('the step is:', 1365, ',the acc is', 0.88328165, ', the cost is', 0.7330684)\n",
            "('the step is:', 1370, ',the acc is', 0.89103854, ', the cost is', 0.70941544)\n",
            "('the step is:', 1375, ',the acc is', 0.8927724, ', the cost is', 0.703645)\n",
            "('the step is:', 1380, ',the acc is', 0.89505386, ', the cost is', 0.70058775)\n",
            "('the step is:', 1385, ',the acc is', 0.8969703, ', the cost is', 0.69857395)\n",
            "('the step is:', 1390, ',the acc is', 0.89258987, ', the cost is', 0.70188797)\n",
            "('the step is:', 1395, ',the acc is', 0.89505386, ', the cost is', 0.70157754)\n",
            "('the step is:', 1400, ',the acc is', 0.8985216, ', the cost is', 0.6975485)\n",
            "('the step is:', 1405, ',the acc is', 0.8921336, ', the cost is', 0.70462155)\n",
            "('the step is:', 1410, ',the acc is', 0.8643913, ', the cost is', 0.74407697)\n",
            "('the step is:', 1415, ',the acc is', 0.8928637, ', the cost is', 0.7046629)\n",
            "('the step is:', 1420, ',the acc is', 0.89760906, ', the cost is', 0.698681)\n",
            "('the step is:', 1425, ',the acc is', 0.8783537, ', the cost is', 0.7242041)\n",
            "('the step is:', 1430, ',the acc is', 0.8998905, ', the cost is', 0.6936984)\n",
            "('the step is:', 1435, ',the acc is', 0.85763824, ', the cost is', 0.7667332)\n",
            "('the step is:', 1440, ',the acc is', 0.8446797, ', the cost is', 0.7865961)\n",
            "('the step is:', 1445, ',the acc is', 0.88966966, ', the cost is', 0.71493995)\n",
            "('the step is:', 1450, ',the acc is', 0.88875705, ', the cost is', 0.7177346)\n",
            "('the step is:', 1455, ',the acc is', 0.87598103, ', the cost is', 0.7333203)\n",
            "('the step is:', 1460, ',the acc is', 0.8700493, ', the cost is', 0.7455826)\n",
            "('the step is:', 1465, ',the acc is', 0.8712356, ', the cost is', 0.7411245)\n",
            "('the step is:', 1470, ',the acc is', 0.87442964, ', the cost is', 0.7353461)\n",
            "('the step is:', 1475, ',the acc is', 0.86457384, ', the cost is', 0.75851697)\n",
            "('the step is:', 1480, ',the acc is', 0.67311555, ', the cost is', 1.4612194)\n",
            "('the step is:', 1485, ',the acc is', 0.80908924, ', the cost is', 0.9524073)\n",
            "('the step is:', 1490, ',the acc is', 0.85508305, ', the cost is', 0.84193265)\n",
            "('the step is:', 1495, ',the acc is', 0.87032306, ', the cost is', 0.7832866)\n",
            "('the shape of cnn output features', (43832, 14), (43832, 5))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aau_rLS9r1T0",
        "colab_type": "code",
        "outputId": "2c148c0f-e5d6-4355-c58f-b60991a4e2ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "#######RNN\n",
        "tf.reset_default_graph()\n",
        "\n",
        "feature_all=feature_all\n",
        "no_fea=feature_all.shape[-1]\n",
        "print (no_fea)\n",
        "# The input to each LSTM layer must be a 3D\n",
        "# feature_all.reshape(samples-batch size-,time step, features)\n",
        "\n",
        "feature_all =feature_all.reshape([final,1,no_fea])\n",
        "#argmax returns the index with the largest value across axis of a tensor\n",
        "print (tf.argmax(labels_all,1))\n",
        "\n",
        "print(\"labels shape\")\n",
        "print (labels_all.shape)\n",
        "print(\"features shape\")\n",
        "print(feature_all.shape)\n",
        "\n",
        "# middle_number=21000\n",
        "feature_training =feature_all[0:middle_number]\n",
        "feature_testing =feature_all[middle_number:final]\n",
        "label_training =labels_all[0:middle_number]\n",
        "label_testing =labels_all[middle_number:final]\n",
        "# print \"label_testing\",label_testing\n",
        "a=feature_training\n",
        "b=feature_testing\n",
        "print(feature_training.shape)\n",
        "print(feature_testing.shape)\n",
        "#264 dimention vector, that is passed to the next layer \n",
        "nodes=264\n",
        "#Used for Weight regulrization \n",
        "lameda=0.004\n",
        "#learning rate\n",
        "lr=0.005\n",
        "\n",
        "batch_size=final-middle_number\n",
        "train_fea=[]\n",
        "n_group=3\n",
        "for i in range(n_group):\n",
        "    f =a[(0+batch_size*i):(batch_size+batch_size*i)]\n",
        "    train_fea.append(f)\n",
        "  \n",
        "print(\"here\")\n",
        "print (train_fea[0].shape)\n",
        "\n",
        "train_label=[]\n",
        "for i in range(n_group):\n",
        "    f =label_training[(0+batch_size*i):(batch_size+batch_size*i), :]\n",
        "    train_label.append(f)\n",
        "print (train_label[0].shape)\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "n_inputs = no_fea\n",
        "n_steps = 1 # time steps\n",
        "n_hidden1_units = nodes   # neurons in hidden layer\n",
        "n_hidden2_units = nodes\n",
        "n_hidden3_units = nodes\n",
        "n_hidden4_units=nodes\n",
        "n_classes = n_classes\n",
        "\n",
        "# tf Graph input\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Define weights\n",
        "#tf.random_normal: Outputs random values from a normal distribution\n",
        "weights = {\n",
        "\n",
        "'in': tf.Variable(tf.random_normal([n_inputs, n_hidden1_units]), trainable=True),\n",
        "'a': tf.Variable(tf.random_normal([n_hidden1_units, n_hidden1_units]), trainable=True),\n",
        "\n",
        "'hidd2': tf.Variable(tf.random_normal([n_hidden1_units, n_hidden2_units])),\n",
        "'hidd3': tf.Variable(tf.random_normal([n_hidden2_units, n_hidden3_units])),\n",
        "'hidd4': tf.Variable(tf.random_normal([n_hidden3_units, n_hidden4_units])),\n",
        "\n",
        "'out': tf.Variable(tf.random_normal([n_hidden4_units, n_classes]), trainable=True),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "#tf.constant result a 1-D tensor of value 0.1\n",
        "'in': tf.Variable(tf.constant(0.1, shape=[n_hidden1_units])),\n",
        "\n",
        "'hidd2': tf.Variable(tf.constant(0.1, shape=[n_hidden2_units ])),\n",
        "'hidd3': tf.Variable(tf.constant(0.1, shape=[n_hidden3_units])),\n",
        "'hidd4': tf.Variable(tf.constant(0.1, shape=[n_hidden4_units])),\n",
        "\n",
        "'out': tf.Variable(tf.constant(0.1, shape=[n_classes ]), trainable=True)\n",
        "}\n",
        "\n",
        "\n",
        "def RNN(X, weights, biases):\n",
        "    # hidden layer for input to cell\n",
        "    ########################################\n",
        "\n",
        "    # transpose the inputs shape from\n",
        "    X = tf.reshape(X, [-1, n_inputs])\n",
        "\n",
        "    # into hidden\n",
        "    #there are n input and output we take only the last output to feed to the next layer\n",
        "    X_hidd1 = tf.matmul(X, weights['in']) + biases['in']\n",
        "    X_hidd2 = tf.matmul(X_hidd1, weights['hidd2']) + biases['hidd2']\n",
        "    X_hidd3 = tf.matmul(X_hidd2, weights['hidd3']) + biases['hidd3']\n",
        "    X_hidd4 = tf.matmul(X_hidd3, weights['hidd4']) + biases['hidd4']\n",
        "    X_in = tf.reshape(X_hidd4, [-1, n_steps, n_hidden4_units])\n",
        "\n",
        "\n",
        "    # cell\n",
        "    ##########################################\n",
        "\n",
        "    # basic LSTM Cell.\n",
        "    # 1-layer LSTM with n_hidden units.\n",
        "    # creates a LSTM layer and instantiates variables for all gates.\n",
        "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden4_units, forget_bias=1.0, state_is_tuple=True)\n",
        "    # 2nd layer LSTM with n_hidden units.\n",
        "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden4_units, forget_bias=1.0, state_is_tuple=True)\n",
        "    # Adding an additional layer to inprove the accuracy\n",
        "    # RNN cell composed sequentially of multiple simple cells.\n",
        "\n",
        "    lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
        "    # lstm cell is divided into two parts (c_state, h_state)\n",
        "    #Initializing the zero state\n",
        "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "    with tf.variable_scope('lstm1'):\n",
        "        # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
        "        outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
        "\n",
        "    # hidden layer for output as the final results\n",
        "    #############################################\n",
        "    print(\"before\")\n",
        "    print(outputs)\n",
        "    outputs = tf.unstack(tf.transpose(outputs, [1, 0, 2]))    # states is the last outputs\n",
        "    print(\"after\")\n",
        "    print(outputs)\n",
        "    #there are n input and n output we take only the last output to feed to the next layer\n",
        "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "    return results, outputs[-1]\n",
        "\n",
        "#################################################################################################################################################\n",
        "\n",
        "pred,Feature = RNN(x, weights, biases)\n",
        "lamena =lameda\n",
        "l2 = lamena * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())  # L2 loss prevents this overkill neural network to overfit the data\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) + l2  # Softmax loss\n",
        "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
        "    # train_op = tf.train.AdagradOptimizer(l).minimize(cost)\n",
        "    # train_op = tf.train.RMSPropOptimizer(0.00001).minimize(cost)\n",
        "    # train_op = tf.train.AdagradDAOptimizer(0.01).minimize(cost)\n",
        "    # train_op = tf.train.GradientDescentOptimizer(0.00001).minimize(cost)\n",
        "# pred_result =tf.argmax(pred, 1)\n",
        "label_true =tf.argmax(y, 1)\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "confusion_m=tf.confusion_matrix(tf.argmax(y, 1), tf.argmax(pred, 1))\n",
        "#starting sessions\n",
        "\n",
        "  \n",
        "with tf.Session() as sess:\n",
        "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "        init = tf.initialize_all_variables()\n",
        "    else:\n",
        "        init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    saver = tf.train.Saver()\n",
        "    step = 0\n",
        "    \n",
        "    \n",
        "  \n",
        "    #downloaded = drive.CreateFile({'id':'10p_NuiBV2Or2sk6cm0yPLfu9tJ2lXEKg'})\n",
        "    #f2 = downloaded.GetContentString()  \n",
        "      \n",
        "    #filename = \"/home/xiangzhang/scratch/results/rnn_acc.csv\"\n",
        "    #f2 = open(filename, 'wb')\n",
        "    while step < 350:\n",
        "        for i in range(n_group):\n",
        "            sess.run(train_op, feed_dict={\n",
        "                x: train_fea[i],\n",
        "                y: train_label[i],\n",
        "            })\n",
        "        if sess.run(accuracy, feed_dict={x: b,y: label_testing,})>0.96:\n",
        "            print(\n",
        "            \"The lamda is :\", lamena, \", Learning rate:\", lr, \", The step is:\", step, \", The accuracy is: \",\n",
        "            sess.run(accuracy, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            }))\n",
        "\n",
        "            break\n",
        "        if step % 5 == 0:\n",
        "            hh=sess.run(accuracy, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            })\n",
        "            #f2.write(str(hh)+'\\n')\n",
        "            print(\", The step is:\",step,\", The accuracy is:\", hh, \"The cost is :\",sess.run(cost, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            }))\n",
        "        step += 1\n",
        "\n",
        "    ##confusion matrix\n",
        "    feature_0=sess.run(Feature, feed_dict={x: train_fea[0]})\n",
        "    for i in range(1,n_group):\n",
        "        feature_11=sess.run(Feature, feed_dict={x: train_fea[i]})\n",
        "        feature_0=np.vstack((feature_0,feature_11))\n",
        "\n",
        "    print (feature_0.shape)\n",
        "    feature_all_rnn=np.vstack((feature_0,feature_b))\n",
        "\n",
        "    confusion_m=sess.run(confusion_m, feed_dict={\n",
        "                x: b,\n",
        "                y: label_testing,\n",
        "            })\n",
        "    print (confusion_m)\n",
        "    ## predict probility\n",
        "    # pred_prob=sess.run(pred, feed_dict={\n",
        "    #             x: b,\n",
        "    #             y: label_testing,\n",
        "    #         })\n",
        "    # # print pred_prob\n",
        "\n",
        "\n",
        "    #print (\"RNN train time:\", time4 - time3, \"Rnn test time\", time5 - time4, 'RNN total time', time5 - time3)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "Tensor(\"ArgMax:0\", shape=(43832,), dtype=int64)\n",
            "labels shape\n",
            "(43832, 5)\n",
            "features shape\n",
            "(43832, 1, 14)\n",
            "(32874, 1, 14)\n",
            "(10958, 1, 14)\n",
            "here\n",
            "(10958, 1, 14)\n",
            "(10958, 5)\n",
            "before\n",
            "Tensor(\"lstm1/rnn/transpose_1:0\", shape=(10958, 1, 264), dtype=float32)\n",
            "after\n",
            "[<tf.Tensor 'unstack:0' shape=(10958, 264) dtype=float32>]\n",
            "(', The step is:', 0, ', The accuracy is:', 0.4022632, 'The cost is :', 559.09827)\n",
            "(', The step is:', 5, ', The accuracy is:', 0.5084869, 'The cost is :', 504.07495)\n",
            "(', The step is:', 10, ', The accuracy is:', 0.56096005, 'The cost is :', 453.01297)\n",
            "(', The step is:', 15, ', The accuracy is:', 0.5812192, 'The cost is :', 406.50662)\n",
            "(', The step is:', 20, ', The accuracy is:', 0.61297685, 'The cost is :', 364.58786)\n",
            "(', The step is:', 25, ', The accuracy is:', 0.6171747, 'The cost is :', 326.86667)\n",
            "(', The step is:', 30, ', The accuracy is:', 0.618361, 'The cost is :', 292.9632)\n",
            "(', The step is:', 35, ', The accuracy is:', 0.66344225, 'The cost is :', 262.45227)\n",
            "(', The step is:', 40, ', The accuracy is:', 0.6743931, 'The cost is :', 235.16617)\n",
            "(', The step is:', 45, ', The accuracy is:', 0.7129038, 'The cost is :', 210.6563)\n",
            "(', The step is:', 50, ', The accuracy is:', 0.7165541, 'The cost is :', 188.71959)\n",
            "(', The step is:', 55, ', The accuracy is:', 0.724311, 'The cost is :', 169.0978)\n",
            "(', The step is:', 60, ', The accuracy is:', 0.7521446, 'The cost is :', 151.4699)\n",
            "(', The step is:', 65, ', The accuracy is:', 0.7635518, 'The cost is :', 135.69272)\n",
            "(', The step is:', 70, ', The accuracy is:', 0.78563607, 'The cost is :', 121.50976)\n",
            "(', The step is:', 75, ', The accuracy is:', 0.78289837, 'The cost is :', 108.8592)\n",
            "(', The step is:', 80, ', The accuracy is:', 0.8221391, 'The cost is :', 97.458244)\n",
            "(', The step is:', 85, ', The accuracy is:', 0.82934844, 'The cost is :', 87.26652)\n",
            "(', The step is:', 90, ', The accuracy is:', 0.8524366, 'The cost is :', 78.11228)\n",
            "(', The step is:', 95, ', The accuracy is:', 0.8640263, 'The cost is :', 69.918236)\n",
            "(', The step is:', 100, ', The accuracy is:', 0.88328165, 'The cost is :', 62.57781)\n",
            "(', The step is:', 105, ', The accuracy is:', 0.8921336, 'The cost is :', 56.008358)\n",
            "(', The step is:', 110, ', The accuracy is:', 0.91157144, 'The cost is :', 50.102726)\n",
            "(', The step is:', 115, ', The accuracy is:', 0.91832453, 'The cost is :', 44.828293)\n",
            "(', The step is:', 120, ', The accuracy is:', 0.9329257, 'The cost is :', 40.09755)\n",
            "(', The step is:', 125, ', The accuracy is:', 0.94132143, 'The cost is :', 35.864807)\n",
            "(', The step is:', 130, ', The accuracy is:', 0.95135975, 'The cost is :', 32.076336)\n",
            "(', The step is:', 135, ', The accuracy is:', 0.9572915, 'The cost is :', 28.693274)\n",
            "('The lamda is :', 0.004, ', Learning rate:', 0.005, ', The step is:', 140, ', The accuracy is: ', 0.9616718)\n",
            "(32874, 264)\n",
            "[[2084   25   13   18   14]\n",
            " [  23 2052   29   32   27]\n",
            " [  22   20 2162   31   10]\n",
            " [  17   23   24 2101   23]\n",
            " [  15   30    8   16 2139]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1ACrN-dr-O-",
        "colab_type": "code",
        "outputId": "dcfdbfe9-b3fa-4ef0-8283-96be54612ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##AE\n",
        "print (feature_all_rnn.shape, feature_all_cnn.shape)\n",
        "print (feature_all_rnn, feature_all_cnn)\n",
        "# stacks the featurese from RNN and CNN in a horizontal stack \n",
        "feature_all=np.hstack((feature_all_rnn,psd))\n",
        "feature_all=np.hstack((feature_all,feature_all_cnn))\n",
        "print(psd.shape, feature_all.shape)\n",
        "no_fea=feature_all.shape[-1]\n",
        "\n",
        "# feature_all =feature_all.reshape([28000,1,no_fea])\n",
        "print (labels_all.shape)\n",
        "\n",
        "feature_training =feature_all[0:middle_number]\n",
        "feature_testing =feature_all[middle_number:final]\n",
        "label_training =labels_all[0:middle_number]\n",
        "label_testing =labels_all[middle_number:final]\n",
        "# print \"label_testing\",label_testing\n",
        "a=feature_training\n",
        "b=feature_testing\n",
        "feature_all=feature_all\n",
        "print(feature_all.shape)\n",
        "\n",
        "train_fea=feature_all[0:middle_number]\n",
        "\n",
        "#dividing the input into three groups\n",
        "group=3\n",
        "display_step = 10\n",
        "#An epoch is a full iteration over samples!!!! training cycle \n",
        "training_epochs = 400\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 800 # 1st layer num features, should be times of 8\n",
        "\n",
        "\n",
        "n_hidden_2=100\n",
        "\n",
        "n_input_ae = no_fea # MNIST data input (img shape: 28*28)\n",
        "\n",
        "# tf Graph input (only pictures)\n",
        "X = tf.placeholder(\"float\", [None, n_input_ae])\n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.random_normal([n_input_ae, n_hidden_1])),\n",
        "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #NOT USED !!!\n",
        "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
        "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input_ae])),\n",
        "}\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'decoder_b2': tf.Variable(tf.random_normal([n_input_ae])),\n",
        "}\n",
        "\n",
        "\n",
        "# Building the encoder\n",
        "def encoder(x):\n",
        "    # Encoder Hidden layer with sigmoid activation #1\n",
        "    #Sigmoid function outputs in the range (0, 1), it makes it ideal for binary classification problems\n",
        "    #there are n input and output we take only the last output to feed to the next layer\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
        "                                   biases['encoder_b1']))\n",
        "    return layer_1\n",
        "\n",
        "\n",
        "# Building the decoder\n",
        "def decoder(x):\n",
        "    # Encoder Hidden layer with sigmoid activation #1\n",
        "    #Sigmoid function outputs in the range (0, 1), it makes it ideal for binary classification problems\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h2']),\n",
        "                                   biases['decoder_b2']))\n",
        "    return layer_1\n",
        "\n",
        "for ll in range(1):\n",
        "    learning_rate = 0.2\n",
        "    for ee in range(1):\n",
        "        # Construct model\n",
        "        encoder_op = encoder(X)\n",
        "        decoder_op = decoder(encoder_op)\n",
        "        # Prediction\n",
        "        y_pred = decoder_op\n",
        "        # Targets (Labels) are the input data, as the auto encoder tries to make output as similar as possible to the input.\n",
        "        y_true = X\n",
        "\n",
        "        # Define loss and optimizer, minimize the squared error\n",
        "        cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
        "        # cost = tf.reduce_mean(tf.pow(y_true, y_pred))\n",
        "        optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "        # Initializing the variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Launch the graph\n",
        "        # saves and restore variables\n",
        "        saver = tf.train.Saver()\n",
        "        with tf.Session() as sess1:\n",
        "            sess1.run(init)\n",
        "            saver = tf.train.Saver()\n",
        "            # Training cycle\n",
        "            for epoch in range(training_epochs):\n",
        "                # Loop over all batches\n",
        "                for i in range(group):\n",
        "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
        "                    _, c = sess1.run([optimizer, cost], feed_dict={X: a})\n",
        "                # Display logs per epoch step\n",
        "                if epoch % display_step == 0:\n",
        "                    print(\"Epoch:\", '%04d' % (epoch+1),\n",
        "                          \"cost=\", \"{:.9f}\".format(c))\n",
        "            print(\"Optimization Finished!\")\n",
        "            a = sess1.run(encoder_op, feed_dict={X: a})\n",
        "            b = sess1.run(encoder_op, feed_dict={X: b})\n",
        "            \n",
        "print(\"Matrix after transformation\")\n",
        "print(a)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((43832, 264), (43832, 120))\n",
            "(array([[-9.6957182e-04,  2.9552106e-03,  3.2806532e-03, ...,\n",
            "        -6.8496552e-04, -3.9453232e-03,  1.9995515e-01],\n",
            "       [-3.6120284e-03, -5.8563795e-02, -1.3408362e-03, ...,\n",
            "        -1.5208798e-05,  2.2732781e-02, -3.5859668e-03],\n",
            "       [ 5.1081521e-03, -2.7070643e-02, -1.6528134e-03, ...,\n",
            "        -7.8784265e-05,  2.9214388e-02,  4.4486221e-02],\n",
            "       ...,\n",
            "       [-4.2094134e-02,  1.8548071e-02, -3.8419608e-02, ...,\n",
            "         4.5004554e-02, -1.9547336e-02,  1.8644926e-01],\n",
            "       [-5.6310151e-02,  3.2972444e-03, -5.3486720e-02, ...,\n",
            "         6.8770811e-02, -3.4729261e-02,  1.1582380e-01],\n",
            "       [-5.6310151e-02,  3.2972444e-03, -5.3486720e-02, ...,\n",
            "         6.8770811e-02, -3.4729261e-02,  1.1582380e-01]], dtype=float32), array([[8.6662173e-03, 7.9154968e-04, 0.0000000e+00, ..., 9.7833455e-01,\n",
            "        7.8165233e-03, 0.0000000e+00],\n",
            "       [1.4568296e-01, 4.6424568e-03, 1.7881393e-07, ..., 8.4560454e-02,\n",
            "        1.8982619e-02, 0.0000000e+00],\n",
            "       [1.8471479e-03, 2.9802322e-07, 0.0000000e+00, ..., 1.3706684e-03,\n",
            "        2.4346292e-02, 0.0000000e+00],\n",
            "       ...,\n",
            "       [2.8834397e-01, 6.1094761e-06, 0.0000000e+00, ..., 5.6922436e-06,\n",
            "        3.9757192e-03, 0.0000000e+00],\n",
            "       [6.9569796e-02, 8.1594884e-03, 0.0000000e+00, ..., 7.0882761e-01,\n",
            "        8.1761777e-01, 0.0000000e+00],\n",
            "       [6.9569796e-02, 8.1594884e-03, 0.0000000e+00, ..., 7.0882761e-01,\n",
            "        8.1761777e-01, 0.0000000e+00]], dtype=float32))\n",
            "((43832, 8), (43832, 392))\n",
            "(43832, 5)\n",
            "(43832, 392)\n",
            "('Epoch:', '0001', 'cost=', '0.504861891')\n",
            "('Epoch:', '0011', 'cost=', '0.495848805')\n",
            "('Epoch:', '0021', 'cost=', '0.452824324')\n",
            "('Epoch:', '0031', 'cost=', '0.294870764')\n",
            "('Epoch:', '0041', 'cost=', '0.139512315')\n",
            "('Epoch:', '0051', 'cost=', '0.092213996')\n",
            "('Epoch:', '0061', 'cost=', '0.069353536')\n",
            "('Epoch:', '0071', 'cost=', '0.049240667')\n",
            "('Epoch:', '0081', 'cost=', '0.036423970')\n",
            "('Epoch:', '0091', 'cost=', '0.031216973')\n",
            "('Epoch:', '0101', 'cost=', '0.029169772')\n",
            "('Epoch:', '0111', 'cost=', '0.028167062')\n",
            "('Epoch:', '0121', 'cost=', '0.027851921')\n",
            "('Epoch:', '0131', 'cost=', '0.027824966')\n",
            "('Epoch:', '0141', 'cost=', '0.027686965')\n",
            "('Epoch:', '0151', 'cost=', '0.027615093')\n",
            "('Epoch:', '0161', 'cost=', '0.027414305')\n",
            "('Epoch:', '0171', 'cost=', '0.027463753')\n",
            "('Epoch:', '0181', 'cost=', '0.027233407')\n",
            "('Epoch:', '0191', 'cost=', '0.025871737')\n",
            "('Epoch:', '0201', 'cost=', '0.026161568')\n",
            "('Epoch:', '0211', 'cost=', '0.025633331')\n",
            "('Epoch:', '0221', 'cost=', '0.025014592')\n",
            "('Epoch:', '0231', 'cost=', '0.024139462')\n",
            "('Epoch:', '0241', 'cost=', '0.023569787')\n",
            "('Epoch:', '0251', 'cost=', '0.023395266')\n",
            "('Epoch:', '0261', 'cost=', '0.022319887')\n",
            "('Epoch:', '0271', 'cost=', '0.021667184')\n",
            "('Epoch:', '0281', 'cost=', '0.022358296')\n",
            "('Epoch:', '0291', 'cost=', '0.021221070')\n",
            "('Epoch:', '0301', 'cost=', '0.020406870')\n",
            "('Epoch:', '0311', 'cost=', '0.019308880')\n",
            "('Epoch:', '0321', 'cost=', '0.018732822')\n",
            "('Epoch:', '0331', 'cost=', '0.017447434')\n",
            "('Epoch:', '0341', 'cost=', '0.016646445')\n",
            "('Epoch:', '0351', 'cost=', '0.015865494')\n",
            "('Epoch:', '0361', 'cost=', '0.015597353')\n",
            "('Epoch:', '0371', 'cost=', '0.014433620')\n",
            "('Epoch:', '0381', 'cost=', '0.014200311')\n",
            "('Epoch:', '0391', 'cost=', '0.013671458')\n",
            "Optimization Finished!\n",
            "Matrix after transformation\n",
            "[[1.4066696e-05 4.1723251e-07 3.3408403e-05 ... 3.9470196e-03\n",
            "  8.4123313e-03 6.1571598e-05]\n",
            " [4.5895576e-06 7.9870224e-06 2.2926927e-03 ... 1.0916591e-04\n",
            "  3.8861245e-02 4.4941902e-05]\n",
            " [1.6644001e-02 1.5124291e-02 4.6277046e-04 ... 6.8783760e-05\n",
            "  4.1485131e-03 8.8415444e-03]\n",
            " ...\n",
            " [7.5211179e-01 1.2617350e-02 1.4898181e-04 ... 1.9973516e-04\n",
            "  3.7789345e-05 1.3504624e-03]\n",
            " [8.3347166e-01 8.1211329e-05 8.0466270e-07 ... 3.5068393e-04\n",
            "  4.7743320e-05 1.5497208e-06]\n",
            " [3.4057915e-02 8.1440806e-04 8.1983805e-03 ... 0.0000000e+00\n",
            "  7.2807074e-05 9.2983246e-06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN_I3sj1sNcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##XGBoost\n",
        "import xgboost as xgb\n",
        "xg_train = xgb.DMatrix(a, label=np.argmax(label_training,1))\n",
        "xg_test = xgb.DMatrix(b, label=np.argmax(label_testing,1))\n",
        "\n",
        "# setup parameters for xgboost\n",
        "param = {}\n",
        "# use softmax multi-class classification\n",
        "param['objective'] = 'multi:softprob' # can I replace softmax by SVM??\n",
        "# softprob produce a matrix with probability value of each class\n",
        "# scale weight of positive examples\n",
        "param['eta'] = 0.5\n",
        "\n",
        "param['max_depth'] = 6\n",
        "param['silent'] = 1\n",
        "param['nthread'] = 4\n",
        "param['subsample']=0.9\n",
        "# param['lambda']=1\n",
        "param['num_class'] =n_classes\n",
        "\n",
        "\n",
        "\n",
        "#np.set_printoptions(threshold=np.nan)\n",
        "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
        "num_round = 10\n",
        "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
        "pred = bst.predict( xg_test );\n",
        "print(\"Prediction\")\n",
        "print(pred)\n",
        "#\n",
        "#print ('predicting, classification error=%f' % ((sum( int(pred[i]) != label_testing[i] for i in range(len(label_testing))) / float(len(label_testing)) )))\n",
        "\n",
        "\n",
        "# print (\"CNN train time:\", time2-time1, \"cnn test time\", time3-time2, 'CNN total time', time3-time1)\n",
        "# print (\"RNN train time:\", time4 - time3, \"Rnn test time\", time5 - time4, 'RNN total time', time5 - time3)\n",
        "# print (\"AE train time:\", time6 - time5, \"AE test time\", time7 - time6, 'AE total time', time7 - time5)\n",
        "# print (\"XGB train time:\", time8 - time7, \"XGB test time\", time9 - time8, 'XGB total time', time9 - time7)\n",
        "# print 'total train time', time2-time1+time4 - time3+time6 - time5+time8 - time7, 'total test time',time3-time2+time5 - time4+time7 - time6+time9 - time8, 'total run time', time9-time1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm2avC2z7iv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bst.save_model('0001.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzxdsSe79RnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dump model\n",
        "bst.dump_model('dump.raw.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgxV1-mr4sCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('predicting, classification error=%f' % ((sum( int(pred[i][j]) != label_testing[i][j] \n",
        "                                                     for i in range((label_testing.shape[0]))\n",
        "                                                    for j in range (label_testing.shape[1])) / float(len(label_testing)) )))\n",
        "\n",
        "intent_labeling = np.array(['','eye_closed', 'left_hand', 'right_hand', 'both_hands', 'both_feet'])\n",
        "pred_argmax = np.argmax(pred,1)\n",
        "print(pred_argmax.shape)\n",
        "print(pred_argmax)\n",
        "copy_pred = np.empty(pred.shape, dtype=object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4jqhUU4F4JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (pred.shape[0]):\n",
        "  copy_pred[i] = intent_labeling[pred_argmax[i]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHlumyjOcnfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(copy_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBsBcTI1iCKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import loadtxt\n",
        "import pickle\n",
        "\n",
        "# save model to file\n",
        "pickle.dump(bst, open(\"pima.pickle.dat\", \"wb\"))\n",
        "\n",
        "#save model to drive\n",
        "model_file = drive.CreateFile({'title' : 'pima.pickle.dat'})\n",
        "model_file.SetContentFile('pima.pickle.dat')      \n",
        "model_file.Upload()\n",
        "#download to google drive   \n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGmquIkZiHxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_obj = drive.CreateFile({'id': u'1guojyvdQwP9njUKN0Ha4Fg0SLq5XXnAE'})\n",
        "file_obj.GetContentFile('pima.pickle.dat')\n",
        "\n",
        "# load model from file\n",
        "loaded_model = pickle.load(open(\"pima.pickle.dat\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eucN0Jq2iMX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = loaded_model.predict( xg_test );\n",
        "print(\"Prediction\")\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gVmi_HDrvTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "xg_test = np.array([[4614.7272722658, 4374.6666662292, 4605.9696965091, 4614.2121207507, \n",
        "              4303.0606056303, 4345.8181813836, 4252.5757571505, 4826.969696487, \n",
        "              4701.2727268026, 4173.7575753402, 5042.3030297988, 5108.7575752467, \n",
        "              4764.1212116448005, 4724.4545449821]])\n",
        "\n",
        "xg_train = np.array([[4614.7272722658, 4374.6666662292, 4605.9696965091, 4614.2121207507, \n",
        "              4303.0606056303, 4345.8181813836, 4252.5757571505, 4826.969696487, \n",
        "              4701.2727268026, 4173.7575753402, 5042.3030297988, 5108.7575752467, \n",
        "              4764.1212116448005, 4724.4545449821]])\n",
        "\n",
        "dummy_label_training = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
        "print(xg_train.shape)\n",
        "#xgboost only accepts 2D numpy arrays so we need to reshape it\n",
        "xg_train = xgb.DMatrix(xg_train, label=dummy_label_training)\n",
        "xg_test = xgb.DMatrix(xg_test, label=dummy_label_training)\n",
        "\n",
        "#setup parameters for xgboost\n",
        "param = {}\n",
        "# use softmax multi-class classification\n",
        "param['objective'] = 'multi:softprob' # can I replace softmax by SVM??\n",
        "# softprob produce a matrix with probability value of each class\n",
        "# scale weight of positive examples\n",
        "param['eta'] = 0.5\n",
        "\n",
        "param['max_depth'] = 6\n",
        "param['silent'] = 1\n",
        "param['nthread'] = 4\n",
        "param['subsample']=0.9\n",
        "# param['lambda']=1\n",
        "param['num_class'] = 6\n",
        "\n",
        "\n",
        "\n",
        "#np.set_printoptions(threshold=np.nan)\n",
        "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
        "num_round = 1\n",
        "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
        "pred = bst.predict( xg_test );"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge1tDfnozGOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "xg_test = np.array([[4614.7272722658, 4374.6666662292, 4605.9696965091, 4614.2121207507, \n",
        "              4303.0606056303, 4345.8181813836, 4252.5757571505, 4826.969696487, \n",
        "              4701.2727268026, 4173.7575753402, 5042.3030297988, 5108.7575752467, \n",
        "              4764.1212116448005, 4724.4545449821]])\n",
        "xg_test = xgb.DMatrix(xg_test)\n",
        "f_names = loaded_model.feature_names\n",
        "print(f_names)\n",
        "print(xg_test.feature_names)\n",
        "pred = loaded_model.predict( xg_test );\n",
        "print(\"Prediction\")\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}